{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables in Snorkel: Extracting Attributes from Spec Sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: `Candidate` Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Training `Corpus`\n",
    "\n",
    "First, we will load the Training `Corpus` that we preprocessed in Part I:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus (Hardware Training) contains 8 Documents\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Corpus\n",
    "from snorkel.utils import get_ORM_instance\n",
    "\n",
    "corpus = get_ORM_instance(Corpus, session, 'Hardware Training')\n",
    "print \"%s contains %d Documents\" % (corpus, len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a `Candidate` Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our candidates to be a binary relation with a part number and a temperature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Part_Temp = candidate_subclass('Part_Temp', ['part','temp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining `ContextSpaces`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special context space objects are made for both Parts and Temperatures because of the unique needs of this dataset. \n",
    "\n",
    "In the `OmniNgramsTemp` object, any \"dash-like\" character (minus sign, em dash, en dash, etc.) is converted to a simple dash, and spaces between dashes and numbers are removed. This improves our ability to interpret the numbers downstream. We anticipate all temperatures being expressed in no more than two n-grams, so we set `n_max=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hardware_utils import OmniNgramsTemp\n",
    "\n",
    "temp_ngrams = OmniNgramsTemp(n_max=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OmniNgramsPart` is more complex. In it, we \"expand\" part numbers in 3 ways:\n",
    "\n",
    "1. Part number ranges are split into individual parts\n",
    "    * e.g., BC548-BC550 -> BC548, BC549, BC550\n",
    "2. Part number suffix groups are enumerated\n",
    "    * e.g., BC548A/B/C -> BC548A, BC548B, BC548C\n",
    "3. Base part numbers are expanded to include more specific variants:\n",
    "    * e.g., (if BC548A/B appears elsewhere in the document), BC548 -> BC548, BC548A, BC548B\n",
    "\n",
    "The third expansion mode requires prior knowledge about the other part numbers that occur in the document. This information can be obtained from a preprocessing pass of the data sheets, or with a gold dictionary, which we use here.\n",
    "\n",
    "These expansion operations allow for part numbers that don't occur verbatim anywhere in the document to still be discovered and matched using the same `Matcher` objects as simple part number mentions. Wherever this expansion occurs, an `ImplicitSpan` object is created instead of a regular `Span` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from hardware_utils import get_gold_dict\n",
    "from collections import defaultdict\n",
    "\n",
    "gold_file = os.environ['SNORKELHOME'] + '/tutorials/tables/data/hardware/hardware_gold.csv'\n",
    "gold_parts = get_gold_dict(gold_file, doc_on=True, part_on=True, val_on=False)\n",
    "parts_by_doc = defaultdict(set)\n",
    "for part in gold_parts:\n",
    "    parts_by_doc[part[0]].add(part[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, for example, that the document 'VISHS23888-1' has fourteen part numbers mentioned in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('VISHS23888-1',\n",
       " {'BC546',\n",
       "  'BC546A',\n",
       "  'BC546B',\n",
       "  'BC547',\n",
       "  'BC547A',\n",
       "  'BC547B',\n",
       "  'BC547C',\n",
       "  'BC548',\n",
       "  'BC548A',\n",
       "  'BC548B',\n",
       "  'BC548C',\n",
       "  'BC549',\n",
       "  'BC549B',\n",
       "  'BC549C'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_by_doc.items()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hardware_utils import OmniNgramsPart\n",
    "\n",
    "part_ngrams = OmniNgramsPart(parts_by_doc=parts_by_doc, n_max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining `Matchers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define matchers, which filter the produced `Span` objects.\n",
    "\n",
    "The `Matcher` for parts looks for a match with any of a number of standard naming protocol regexes for transistors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import RegexMatchSpan, Union\n",
    "\n",
    "eeca_matcher = RegexMatchSpan(rgx='([b]{1}[abcdefklnpqruyz]{1}[\\swxyz]?[0-9]{3,5}[\\s]?[A-Z\\/]{0,5}[0-9]?[A-Z]?([-][A-Z0-9]{1,7})?([-][A-Z0-9]{1,2})?)')\n",
    "jedec_matcher = RegexMatchSpan(rgx='([123]N\\d{3,4}[A-Z]{0,5}[0-9]?[A-Z]?)')\n",
    "jis_matcher = RegexMatchSpan(rgx='(2S[abcdefghjkmqrstvz]{1}[\\d]{2,4})')\n",
    "others_matcher = RegexMatchSpan(rgx='((NSVBC|SMBT|MJ|MJE|MPS|MRF|RCA|TIP|ZTX|ZT|TIS|TIPL|DTC|MMBT|PZT){1}[\\d]{2,4}[A-Z]{0,3}([-][A-Z0-9]{0,6})?([-][A-Z0-9]{0,1})?)')\n",
    "parts_matcher = Union(eeca_matcher, jedec_matcher, jis_matcher, others_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Matcher` for temperatures looks for temperatures in the set {-50, -55, -60, -65, -70} which we know by experience covers nearly all transistors in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import RegexMatchSpan\n",
    "\n",
    "temp_matcher = RegexMatchSpan(rgx=r'-[5-7][05]', longest_match_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the `CandidateExtractor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine these `ContextSpaces` and `Matchers` to form a `CandidateExtractor`, which we can now apply to our `Corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import CandidateExtractor\n",
    "\n",
    "ce = CandidateExtractor(Part_Temp, [part_ngrams, temp_ngrams], [parts_matcher, temp_matcher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 11.5 s, sys: 145 ms, total: 11.6 s\n",
      "Wall time: 11.8 s\n",
      "Candidate Set (Hardware Training Candidates) contains 6571 Candidates\n"
     ]
    }
   ],
   "source": [
    "%time train = ce.extract(corpus.documents, 'Hardware Training Candidates', session)\n",
    "print \"%s contains %d Candidates\" % (train, len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part_Temp(ImplicitSpan(\"2N3906\", parent=653, words=[0,0], position=[0]), ImplicitSpan(\"-50\", parent=11554, words=[0,0], position=[0]))\n"
     ]
    }
   ],
   "source": [
    "print train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the extracted candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.add(train)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Set (Hardware Training Candidates) contains 6571 Candidates\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "from snorkel.utils import get_ORM_instance\n",
    "\n",
    "train = get_ORM_instance(CandidateSet, session, 'Hardware Training Candidates')\n",
    "print \"%s contains %d Candidates\" % (train, len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating for Development Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Candidates from Corpus (Hardware Development)\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1.23 s, sys: 24.8 ms, total: 1.26 s\n",
      "Wall time: 1.29 s\n",
      "Candidate Set (Hardware Development Candidates) contains 57 Candidates\n"
     ]
    }
   ],
   "source": [
    "for corpus_name in ['Hardware Development']:\n",
    "    corpus = get_ORM_instance(Corpus, session, corpus_name)\n",
    "    print \"Extracting Candidates from %s\" % corpus\n",
    "    %time candidates = ce.extract(corpus.documents, corpus_name + ' Candidates', session)\n",
    "    session.add(candidates)\n",
    "    print \"%s contains %d Candidates\" % (candidates, len(candidates))\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in Part 3, we will load `Labels` for each of our `Candidates` so that we can evaluate performance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
