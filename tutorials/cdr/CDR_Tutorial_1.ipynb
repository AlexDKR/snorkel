{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chemical-Disease Relation (CDR) Tutorial\n",
    "\n",
    "In this example, we'll be writing an application to extract *mentions of* **chemical-induced-disease relationships** from Pubmed abstracts, as per the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  This tutorial will show off some of the more advanced features of Snorkel, so we'll assume you've followed the Intro tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Description\n",
    "\n",
    "The CDR task is comprised of three sets of 500 documents each, called training, development, and test. A document consists of the title and abstract of an article from [PubMed](https://www.ncbi.nlm.nih.gov/pubmed/), an archive of biomedical and life sciences journal literature. The documents have been hand-annotated with\n",
    "* Mentions of chemicals and diseases along with their [MESH](https://meshb.nlm.nih.gov/#/fieldSearch) IDs, canonical IDs for medical entities. For example, mentions of \"warfarin\" in two different documents will have the same ID.\n",
    "* Chemical-disease relations at the document-level. That is, if some piece of text in the document implies that a chemical with MESH ID `X` induces a disease with MESH ID `Y`, the document will be annotated with `Relation(X, Y)`.\n",
    "\n",
    "The goal is to extract the document-level relations on the test set (without accessing the entity or relation annotations). For this tutorial, we make the following assumptions and alterations to the task:\n",
    "* We discard all of the entity mention annotations and assume we have access to a state-of-the-art entity tagger (see Part I) to identify chemical and disease mentions, and link them to their canonical IDs.\n",
    "* We shuffle the training and development sets a bit, producing a new training set with 900 documents and a new development set with 100 documents. We discard the training set relation annotations, but keep the development set to evaluate our labeling functions and extraction model.\n",
    "* We evaluate the task at the mention-level, rather than the document-level. We will convert the document-level relation annotations to mention-level by simply saying that a mention pair `(X, Y)` in document `D` if `Relation(X, Y)` was hand-annotated at the document-level for `D`.\n",
    "\n",
    "In effect, the only inputs to this application are the plain text of the documents, a pre-trained entity tagger, and a small development set of annotated documents. This is representative of many information extraction tasks, and Snorkel is the perfect tool to bootstrap the extraction process with weak supervision. Let's get going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Corpus Preprocessing\n",
    "\n",
    "### Before starting, make sure to run the download_data.sh script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "os.environ['SNORKELDB'] = 'postgres:///cdr-structure-learning-2'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a `DocPreprocessor`\n",
    "\n",
    "We'll start by defining a `DocPreprocessor` object to read in Pubmed abstracts from [Pubtator]([Pubtator](http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/PubTator/index.cgi). There some extra annotation information in the file, while we'll skip for now. We'll use the `XMLMultiDocPreprocessor` class, which allows us to use [XPath queries](https://en.wikipedia.org/wiki/XPath) to specify the relevant sections of the XML format.\n",
    "\n",
    "Note that we are newline-concatenating text from the title and abstract together for simplicity, but if we wanted to, we could easily extend the `DocPreprocessor` classes to preserve information about document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from snorkel.parser import XMLMultiDocPreprocessor\n",
    "\n",
    "# The following line is for testing only. Feel free to ignore it.\n",
    "file_path = 'data/CDR.BioC.small.xml' if 'CI' in os.environ else 'data/CDR.BioC.xml'\n",
    "\n",
    "doc_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a `CorpusParser`\n",
    "\n",
    "Similar to the Intro tutorial, we'll now construct a `CorpusParser` using the preprocessor we just defined. However, this one has an extra ingredient: an entity tagger. [TaggerOne](https://www.ncbi.nlm.nih.gov/pubmed/27283952) is a popular entity tagger for PubMed, so we went ahead and preprocessed its tags on the CDR corpus for you. The function `TaggerOneTagger.tag` (in `utils.py`) tags sentences with mentions of chemicals and diseases. We'll use these tags to extract candidates in Part II. The tags are stored in `Sentence.entity_cids` and `Sentence.entity_types`, which are analog to `Sentence.words`.\n",
    "\n",
    "Recall that in the wild, we wouldn't have the manual labels included with the CDR data, and we'd have to use an automated tagger (like TaggerOne) to tag entity mentions. That's what we're doing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "from utils import TaggerOneTagger\n",
    "\n",
    "tagger_one = TaggerOneTagger()\n",
    "corpus_parser = CorpusParser(fn=tagger_one.tag)\n",
    "corpus_parser.apply(list(doc_preprocessor), clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 143381\n",
      "Sentences: 1543259\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Sentences:\", session.query(Sentence).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating data from `snorkel-biocorpus-new`\n",
    "\n",
    "First, run the script `transfer_tables.sh`.\n",
    "\n",
    "Then--obviously not the cleanest / most efficient way to do this--but..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a mapping of old ids -> new ids in memory\n",
    "\n",
    "This will allow us to (a) use `bulk_save_objects` and (b) insert documents and sentences separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = session.execute(\"\"\"SELECT id FROM context_filtered_kw\"\"\")\n",
    "old_ids = [r[0] for r in res.fetchall()]\n",
    "print len(old_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_to_new_ids = {}\n",
    "for i, old_id in enumerate(old_ids):\n",
    "    old_to_new_ids[old_id] = 15502 + i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Insert `Documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = session.execute(\"\"\"\n",
    "    SELECT d.id, d.name, c.stable_id, d.meta\n",
    "    FROM document_filtered_kw d, context_filtered_kw c\n",
    "    WHERE d.id = c.id\"\"\")\n",
    "\n",
    "docs = [Document(id=old_to_new_ids[r.id], name=r.name, stable_id=r.stable_id, meta=r.meta) for r in res.fetchall()]\n",
    "print \"Fetched %s docs...\" % len(docs)\n",
    "\n",
    "session.bulk_save_objects(docs)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Insert `Sentences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = session.execute(\"\"\"\n",
    "    SELECT s.*, c.stable_id\n",
    "    FROM sentence_filtered_kw s, context_filtered_kw c\n",
    "    WHERE s.id = c.id\"\"\")\n",
    "\n",
    "rows = res.fetchall()\n",
    "print len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "for r in rows:\n",
    "    sent = Sentence(\n",
    "                id=old_to_new_ids[r.id],\n",
    "                document_id=old_to_new_ids[r.document_id],\n",
    "                position=r.position,\n",
    "                text=r.text,\n",
    "                words=r.words,\n",
    "                char_offsets=r.char_offsets,\n",
    "                lemmas=r.lemmas,\n",
    "                pos_tags=r.pos_tags,\n",
    "                ner_tags=r.ner_tags,\n",
    "                dep_parents=r.dep_parents,\n",
    "                dep_labels=r.dep_labels,\n",
    "                entity_cids=r.entity_cids,\n",
    "                entity_types=r.entity_types,\n",
    "                stable_id=r.stable_id)\n",
    "    sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.bulk_save_objects(sents)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Sentences:\", session.query(Sentence).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Candidate Extraction\n",
    "\n",
    "With the TaggerOne entity tags, candidate extraction is pretty easy! We split into some preset training, development, and test sets. Then we'll use PretaggedCandidateExtractor to extract candidates using the TaggerOne entity tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "TRAINING_SUBSAMPLE = 0.0\n",
    "SCALE_UP_START_ID  = 15502\n",
    "\n",
    "with open('data/doc_ids.pkl', 'rb') as f:\n",
    "    train_ids, dev_ids, test_ids = cPickle.load(f)\n",
    "train_ids, dev_ids, test_ids = set(train_ids), set(dev_ids), set(test_ids)\n",
    "\n",
    "train_sents, dev_sents, test_sents = set(), set(), set()\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "for i, doc in enumerate(docs):\n",
    "    \n",
    "    # Subsample scale-up set at the doc level\n",
    "    r = np.random.random()\n",
    "    \n",
    "    for s in doc.sentences:\n",
    "        if doc.name in train_ids:\n",
    "            train_sents.add(s)\n",
    "        elif doc.name in dev_ids:\n",
    "            dev_sents.add(s)\n",
    "        elif doc.name in test_ids:\n",
    "            test_sents.add(s)\n",
    "        \n",
    "        # New docs we added from scale up set AND SUBSAMPLE\n",
    "        elif doc.id > SCALE_UP_START_ID:\n",
    "            if r < TRAINING_SUBSAMPLE:\n",
    "                train_sents.add(s)\n",
    "        \n",
    "        # Throw error if we're missing something from the core CDR corpus\n",
    "        else:\n",
    "            raise Exception('ID <{0}> not found in any id set'.format(doc.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(train_sents)\n",
    "print len(dev_sents)\n",
    "print len(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Candidate, candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import PretaggedCandidateExtractor\n",
    "\n",
    "candidate_extractor = PretaggedCandidateExtractor(ChemicalDisease, ['Chemical', 'Disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update `context.id` increment value\n",
    "\n",
    "Or else we get IntegrityErrors coming up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = session.execute(\"SELECT MAX(id) FROM context;\")\n",
    "max_cid = res.fetchall()[0][0]\n",
    "print max_cid\n",
    "session.execute(\"ALTER SEQUENCE context_id_seq RESTART WITH %s;\" % (max_cid + 1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, sents in enumerate([train_sents_ss, dev_sents, test_sents]):\n",
    "    candidate_extractor.apply(sents, split=k, check_for_existing=False)\n",
    "    print \"Number of candidates:\", session.query(ChemicalDisease).filter(ChemicalDisease.split == k).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Recall\n",
    "We will briefly discuss the issue of candidate recall. The end-recall of the extraction is effectively upper-bounded by our candidate set: any chemical-disease pair that is present in a document but not identified as a candidate cannot be extracted by our end extraction model. Below are some example reasons for missing a candidate<sup>1</sup>.\n",
    "* The tagger is imperfect, and may miss a chemical or disease mention.\n",
    "* The tagger is imperfect, and may attach an incorrect entity ID to a correctly identified chemical or disease mention. For example, \"stomach pain\" might get attached to the entity ID for \"digestive track infection\" rather than \"stomach illness\".\n",
    "* A relation occurs across multiple sentences. For example, \"**Artery calcification** is more prominient in older populations. It can be induced by **warfarin**.\"\n",
    "\n",
    "If we just look at the set of extractions at the end of this tutorial, we won't be able to account for some false negatives that we missed at the candidate extraction stage. For simplicity, we ignore candidate recall in this tutorial and evaluate our extraction model just on the set of extractions made by the end model. However, when you're developing information extraction applications in the future, it's important to keep candidate recall in mind.\n",
    "\n",
    "<sup>1</sup>Note that these specific issues can be combatted with advanced techniques like noun-phrase chunking to expand the entity mention set, or coreference parsing for cross-sentence candidates. We don't employ these here in order to focus on weak supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Candidate\n",
    "print \"Training candidates:\", session.query(ChemicalDisease).filter(Candidate.split == 0).count()\n",
    "print \"Dev candidates:\", session.query(ChemicalDisease).filter(Candidate.split == 1).count()\n",
    "print \"Test candidates:\", session.query(ChemicalDisease).filter(Candidate.split == 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_candidate = session.query(ChemicalDisease).filter(Candidate.split == 0).all()\n",
    "len(train_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[c.get_cids() for c in train_candidate if '|' in c.get_cids()[0] or '|' in c.get_cids()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([c.get_cids() for c in train_candidate if 'MESH' not in c.get_cids()[0] or 'MESH' not in c.get_cids()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([c.get_cids() for c in train_candidate if 'CHEBI' in c.get_cids()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[c.get_cids() for c in train_candidate if 'MESH' not in c.get_cids()[0] or 'MESH' not in c.get_cids()[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization\n",
    "\n",
    "## Note\n",
    "At this point, ran into a bug where a `ObjectDeletedError` was thrown by SQLAlchemy.  Turns out there are `Candidate` records with no corresponding `ChemicalDisease` record in the DB... just deleting these will fix this, or the quick fix below you to pass in a `cand_class` that the annotator will query instead of `Candidate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.query(Candidate).count() == session.query(ChemicalDisease).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another TODO: clear existing step is _way_ too slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time F_train = featurizer.apply(split=0, parallelism=20)\n",
    "F_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "F_dev  = featurizer.apply_existing(split=1)\n",
    "F_test = featurizer.apply_existing(split=2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
