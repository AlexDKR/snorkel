{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Norm\n",
    "\n",
    "In this example, we'll be writing an application to extract *mentions of* diseases from Pubmed abstracts, using annotations from the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  This tutorial, which has 5 parts, walks through the process of constructing a model to classify _candidate_ disease mentions as either true (i.e., that it is truly a mention of a disease) or false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Candidates + Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Disease = candidate_subclass('Disease', ['disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28087\n",
      "27896\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "train = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Training Candidates').one()\n",
    "print len(train)\n",
    "dev = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Development Candidates').one()\n",
    "print len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28087, 1)\n",
      "(27896, 1)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "\n",
    "label_manager = LabelManager()\n",
    "\n",
    "L_gold_train = label_manager.load(session, train, \"CDR Training Label Set\")\n",
    "print L_gold_train.shape\n",
    "L_gold_dev = label_manager.load(session, dev, \"CDR Development Label Set\")\n",
    "print L_gold_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process / Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureManager\n",
    "\n",
    "feature_manager = FeatureManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time F_train = feature_manager.create(session, train, 'Train Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time F_dev = feature_manager.update(session, dev, 'Train Features', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, load if already processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time F_train = feature_manager.load(session, train, 'Train Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time F_dev = feature_manager.load(session, dev, 'Train Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MESH ID -> CID mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cPickle import load\n",
    "MESH_to_CID = load(open('MESH_to_CID.pkl', 'rb'))\n",
    "diseases    = load(open('diseases.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mesh_to_terms = defaultdict(set)\n",
    "for term, mid in diseases.iteritems():\n",
    "    mesh_to_terms[mid].add(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a canonical dictionary (CD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import CanonicalDictionary\n",
    "cd = CanonicalDictionary(MESH_to_CID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MESH to CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27885 entries\n"
     ]
    }
   ],
   "source": [
    "# Load MESH\n",
    "from utils import load_mesh_raw\n",
    "mesh_entries = load_mesh_raw('data/desc2016.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151006"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add MESH to cd\n",
    "for entry in mesh_entries:\n",
    "    mid, ps, terms = entry\n",
    "    paths = [[p[0]] + p[1:].split('.') for p in ps]\n",
    "    for term in terms:\n",
    "        cd.add_term(term, mid, tree_paths=paths)\n",
    "\n",
    "len(cd.term_to_sids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MEDIC to CD\n",
    "\n",
    "Custom CTD diseases dictionary made from MESH category C + OMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11864 MEDIC entries\n"
     ]
    }
   ],
   "source": [
    "from utils import load_MEDIC, load_mesh_raw\n",
    "medic_entries, MEDIC_to_CID = load_MEDIC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add MESH to cd\n",
    "for entry in medic_entries:\n",
    "    if entry.id.startswith(\"MESH\"):\n",
    "        mid = entry.id.split(\":\")[1]\n",
    "    elif len(entry.parent_ids) > 0 and entry.parent_ids[0].startswith(\"MESH\"):\n",
    "        mid = entry.parent_ids[0].split(\":\")[1]\n",
    "    else:\n",
    "        raise KeyError(entry)\n",
    "    \n",
    "    paths = []\n",
    "    for p in entry.tree_nums:\n",
    "        x = p.split(\"/\")[0]\n",
    "        paths.append([x[0]] + x[1:].split('.'))\n",
    "    \n",
    "    terms = [entry.name] + entry.synonyms\n",
    "    for term in terms:\n",
    "        cd.add_term(term, mid, tree_paths=paths)\n",
    "        \n",
    "len(cd.term_to_sids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add UMLS to CD\n",
    "\n",
    "This may or may not be all of the UMLS... file from Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801566"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('cui2mesh.tsv', 'rb') as f:\n",
    "    for line in f:\n",
    "        term, cui, mid = line.rstrip('\\n').split('\\t')\n",
    "        cd.add_term(term, mid)\n",
    "\n",
    "len(cd.term_to_sids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Add MESH supp?  No paths though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing some multinomial LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Beware of LF rollback bug!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TYPE I LF: Subsets of MESH dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEEN_GLOBAL = defaultdict(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESH exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POS_DEPTH = 3\n",
    "NEG_DEPTH = 3\n",
    "def LFG_CD_match(c, p, key_mod=None, seen_global=None, max_paths_per_sid=1):\n",
    "    \"\"\"\n",
    "    Given a candidate c, some transformed candidate disease phrase p,\n",
    "    and an optional key name modifier key_mod to be appended, return a generator\n",
    "    of key, value pairs\n",
    "    \"\"\"\n",
    "    if p in cd.term_to_sids:\n",
    "        for sid in cd.term_to_sids[p]:\n",
    "            cid   = cd.sid_to_cid[sid] if sid in cd.sid_to_cid else -1\n",
    "            paths = cd.tree_paths[sid]\n",
    "            for path in paths[:max_paths_per_sid]:\n",
    "                    \n",
    "                # NOTE: path may be shorter than max depth if higher up in the tree (e.g. 'cancer', 'ischemia')!\n",
    "                key = \"-\".join(path[:POS_DEPTH]) if cid > 0 else \"-\".join(path[:NEG_DEPTH])\n",
    "                        \n",
    "                # CHECK AGAINST / ADD TO GLOBAL SEEN SET!\n",
    "                # To make sure that relaxations of an LF don't overlap with each other on any given candidate\n",
    "                if seen_global is None or c.id not in seen_global[key]:\n",
    "                    if seen_global is not None:\n",
    "                        seen_global[key].add(c.id)\n",
    "                    if key_mod:\n",
    "                        key += \"-\" + key_mod\n",
    "                    yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LFG_MESH_exact(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return LFG_CD_match(c, p, seen_global=SEEN_GLOBAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================] 100%\n",
      "\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 1min 29s, sys: 17.5 s, total: 1min 46s\n",
      "Wall time: 1min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<28087x382 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 7688 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time L_train = label_manager.create(session, train, 'LF Training Labels -- ALL 2', f=LFG_MESH_exact)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop JJs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_jjs(c):\n",
    "    toks  = []\n",
    "    words = c.disease.get_attrib_tokens()\n",
    "    for i, tag in enumerate(c.disease.get_attrib_tokens('pos_tags')):\n",
    "        if re.match(r'JJ.*', tag) is None:\n",
    "            toks.append(words[i])\n",
    "    return \" \".join(toks).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LFG_MESH_exact_drop_JJs(c):\n",
    "    p = drop_jjs(c)\n",
    "    return LFG_CD_match(c, p, key_mod=\"DJ\", seen_global=SEEN_GLOBAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================] 100%\n",
      "\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 1min 29s, sys: 17.5 s, total: 1min 47s\n",
      "Wall time: 1min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<28087x618 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 10208 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time L_train = label_manager.update(session, train, 'LF Training Labels -- ALL 2', True, LFG_MESH_exact_drop_JJs)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESH TF-IDF cosine match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|V| = 271512\n",
      "CPU times: user 7min 3s, sys: 2.05 s, total: 7min 5s\n",
      "Wall time: 7min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from entity_norm import CanonDictVectorizer\n",
    "\n",
    "# Create a vectorizer based around this \n",
    "cd_vectorizer = CanonDictVectorizer(cd.term_to_sids, other_phrases=[])\n",
    "\n",
    "# Vectorize the dictionary\n",
    "D  = cd_vectorizer.vectorize_phrases(cd.terms)\n",
    "Dt = D.T\n",
    "Dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "POS_DEPTH         = 3\n",
    "NEG_DEPTH         = 3\n",
    "THRESH            = 0.75\n",
    "seen_global       = SEEN_GLOBAL\n",
    "max_paths_per_sid = 1\n",
    "\n",
    "def LFG_CD_cosine_match(c):\n",
    "    p  = c.disease.get_span().lower()\n",
    "    cx = cd_vectorizer.vectorize_phrases([p])\n",
    "    m  = cx * Dt\n",
    "    m  = m.tocoo()\n",
    "\n",
    "    best_match = defaultdict(lambda : (0,None))\n",
    "    for i, s in enumerate(m.data):\n",
    "        if s > THRESH:\n",
    "            j = m.col[i]\n",
    "            t = cd.terms[j]\n",
    "            for sid in cd.term_to_sids[t]:\n",
    "                cid = cd.sid_to_cid[sid] if sid in cd.sid_to_cid else -1\n",
    "                for path in cd.tree_paths[sid][:max_paths_per_sid]:\n",
    "                    key  = \"-\".join(path[:POS_DEPTH]) if cid > 0 else \"-\".join(path[:NEG_DEPTH])\n",
    "                    if s > best_match[key][0]:\n",
    "                        best_match[key] = (s, cid)\n",
    "\n",
    "    for key, x in best_match.iteritems():\n",
    "        s, cid = x\n",
    "        \n",
    "        # CHECK AGAINST / ADD TO GLOBAL SEEN SET!\n",
    "        # To make sure that relaxations of an LF don't overlap with each other on any given candidate\n",
    "        if seen_global is None or c.id not in seen_global[key]:\n",
    "            if seen_global is not None:\n",
    "                seen_global[key].add(c.id)\n",
    "            key += \"-c\"\n",
    "            yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================] 100%\n",
      "\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 28min 23s, sys: 2min 46s, total: 31min 9s\n",
      "Wall time: 30min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<28087x1287 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 33969 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time L_train = label_manager.update(session, train, 'LF Training Labels -- ALL 2', True, LFG_CD_cosine_match)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting in some negative LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from lf_terms import *\n",
    "from snorkel.lf_helpers import get_left_tokens, get_right_tokens\n",
    "from utils import *\n",
    "from Disease_Tagging_Tutorial_LFs import *\n",
    "chemicals = load_chemdner_dictionary()\n",
    "\n",
    "def LF_organs(c):\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens()).lower()\n",
    "    return -1 if phrase in organs else 0      \n",
    "\n",
    "def LF_chemical_name(c):\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens())\n",
    "    return -1 if phrase in chemicals and not phrase.isupper() else 0\n",
    "\n",
    "def LF_bodysym(c):\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens()).lower()\n",
    "    return -1 if phrase in bodysym else 0  \n",
    "\n",
    "def LF_protein_chemical_abbrv(c):\n",
    "    '''Gene/protein/chemical name'''\n",
    "    lemma = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return -1 if re.search(\"\\d+\",lemma) else 0\n",
    "\n",
    "def LF_base_pair_seq(c): \n",
    "    lemma = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return -1 if re.search(\"^[GACT]{2,}$\",lemma) else 0\n",
    "\n",
    "LFs_false = [LF_chemical_name,\n",
    "             LF_organs,\n",
    "             LF_bodysym,\n",
    "             LF_protein_chemical_abbrv,\n",
    "             LF_base_pair_seq,\n",
    "             LF_too_vague,\n",
    "             LF_neg_surfix,\n",
    "             LF_non_common_disease,\n",
    "             LF_non_disease_acronyms,\n",
    "             LF_pos_in,\n",
    "             LF_gene_chromosome_link,\n",
    "             LF_right_window_incomplete,\n",
    "             LF_negative_indicator\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================] 100%\n",
      "\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 5min 32s, sys: 18.3 s, total: 5min 51s\n",
      "Wall time: 5min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<28087x1300 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 47892 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time L_train = label_manager.update(session, train, 'LF Training Labels -- ALL 2', True, LFs_false)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOAD if already computed\n",
    "L_train = label_manager.load(session, train, 'LF Training Labels -- ALL 1')\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running gen. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<28087x1300 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 47892 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import binarize_LF_matrix, get_binarized_score\n",
    "L_train_b = binarize_LF_matrix(L_train)\n",
    "L_train_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training marginals (!= 0.5):\t28087\n",
      "Features:\t\t\t1300\n",
      "================================================================================\n",
      "Begin training for rate=0.1, mu=1e-06\n",
      "\tLearning epoch = 0\tGradient mag. = 0.211986\n",
      "\tLearning epoch = 250\tGradient mag. = 0.255646\n",
      "\tLearning epoch = 500\tGradient mag. = 0.235344\n",
      "\tLearning epoch = 750\tGradient mag. = 0.199824\n",
      "\tLearning epoch = 1000\tGradient mag. = 0.166825\n",
      "\tLearning epoch = 1250\tGradient mag. = 0.140889\n",
      "\tLearning epoch = 1500\tGradient mag. = 0.117636\n",
      "\tLearning epoch = 1750\tGradient mag. = 0.115118\n",
      "\tLearning epoch = 2000\tGradient mag. = 0.108628\n",
      "\tLearning epoch = 2250\tGradient mag. = 0.102782\n",
      "\tLearning epoch = 2500\tGradient mag. = 0.100412\n",
      "\tLearning epoch = 2750\tGradient mag. = 0.099542\n",
      "Final gradient magnitude for rate=0.1, mu=1e-06: 0.098\n",
      "CPU times: user 5.65 s, sys: 28.4 ms, total: 5.68 s\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model = NaiveBayes()\n",
    "%time gen_model.train(L_train_b, n_iter=3000, rate=1e-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P :\t0.89796819788\n",
      "R :\t0.525322997416\n",
      "F1:\t0.662862732312\n"
     ]
    }
   ],
   "source": [
    "yp = gen_model.predict(L_train_b)\n",
    "get_binarized_score(yp, L_gold_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print LF stats...\n",
    "from snorkel.learning import odds_to_prob\n",
    "lfs = L_train.lf_stats(labels=L_gold_train, est_accs=odds_to_prob(gen_model.w))\n",
    "lfs.nlargest(50, \"coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis\n",
    "\n",
    "Current ideas:\n",
    "* Take out 'F' category? [6]\n",
    "* Take out supplemental entries [2]\n",
    "* Don't emit all tree paths?  **This essentially gives many more votes if it has multiple tree paths!!!** [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "N_train = L_gold_train.shape[0]\n",
    "\n",
    "fps = []\n",
    "fns = []\n",
    "for i in range(N_train):\n",
    "    if yp[i] > 0 and L_gold_train[i] < 0:\n",
    "        fps.append(i)\n",
    "    elif yp[i] < 0 and L_gold_train[i] > 0:\n",
    "        fns.append(i)\n",
    "\n",
    "shuffle(fps)\n",
    "shuffle(fns)\n",
    "\n",
    "print len(fps)\n",
    "print len(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "fn_cands = [L_train.get_candidate(i) for i in fns[:100]]\n",
    "svn      = SentenceNgramViewer(fn_cands, session)\n",
    "svn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = svn.get_selected()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.disease.get_attrib_tokens('pos_tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Label\n",
    "session.query(Label).filter(Label.candidate == c).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.gen_learning import odds_to_prob\n",
    "i = L_train.get_row_index(c)\n",
    "\n",
    "for j in L_train.getrow(i).nonzero()[1]:\n",
    "    print L_train.get_key(j), odds_to_prob(gen_model.w[j]), int(L_train[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd.cid_to_sid[1419]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mesh_to_terms[cd.cid_to_sid[1419]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd.term_to_sids['depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, cid in LFG_MESH_exact(c):\n",
    "    print key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, cid in LFG_CD_cosine_match(c):\n",
    "    print key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mt = 0.5\n",
    "m  = m.tocoo()\n",
    "for i, s in enumerate(m.data):\n",
    "    if s > mt:\n",
    "        j = m.col[i]\n",
    "        t = cd.terms[j]\n",
    "        print t, s\n",
    "        for sid in cd.term_to_sids[t]:\n",
    "            cid = cd.sid_to_cid[sid] if sid in cd.sid_to_cid else -1\n",
    "            print \"\\t\", sid, cid\n",
    "            for path in cd.tree_paths[sid]:\n",
    "                key  = \"-\".join(path[:POS_DEPTH]) if cid > 0 else \"-\".join(path[:NEG_DEPTH])\n",
    "                print \"\\t\\t\", key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in m.nonzero()[1]:\n",
    "    t = cd.terms[j]\n",
    "    print t, cd.term_to_sids[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd.sid_to_cid['C566870']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[cd.sid_to_cid[sid] for sid in cd.term_to_sids['toxicity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd.cid_to_sid[5593]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "* _DONE: Re-write the cosine matcher_\n",
    "* _DONE: Switch to MESH 2016_\n",
    "* _DONE: Add in supplementary records!_\n",
    "* **Try labeling in cascading if-then fashion...?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Discriminative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import LogReg\n",
    "\n",
    "train_marginals = gen_model.marginals(L_train_b)\n",
    "\n",
    "disc_model = LogReg()\n",
    "disc_model.train(F_train, train_marginals, n_iter=2000, rate=1e-3, mu=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp = disc_model.predict(F_train)\n",
    "get_binarized_score(yp, L_gold_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp = disc_model.predict(F_dev, b=0.5)\n",
    "get_binarized_score(yp, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(disc_model.marginals(F_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.gen_learning import odds_to_prob\n",
    "plt.hist(odds_to_prob(gen_model.w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train.lf_stats(labels=L_gold_train, est_accs=odds_to_prob(gen_model.w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try overriding with any exact matches...\n",
    "ype = np.zeros(L_gold_train.shape[0])\n",
    "for i,c in enumerate(dev):\n",
    "    if i % 5000 == 0:\n",
    "        print i\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for lf_name, label in LFG_MESH_exact(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    for lf_name, label in LFG_MESH_exact_drop_JJs(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    if neg > 0 and pos == 0:\n",
    "        ype[i] = -1\n",
    "    elif pos > 0 and neg == 0:\n",
    "        ype[i] = 1\n",
    "    else:\n",
    "        ype[i] = yp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_binarized_score(ype, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, getting CIDs in simple heuristic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = L_gold_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp_idxs = [i for i in range(N) if yp[i] == 1 and L_gold_train[i] > 0]\n",
    "pp_idxs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 32\n",
    "for j in L_train.getrow(i).nonzero()[1]:\n",
    "    print L_train.get_key(j), gen_model.w[j], L_train[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 53\n",
    "for j in L_train.getrow(i).nonzero()[1]:\n",
    "    print L_train.get_key(j), gen_model.w[j], L_train[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_gold_train[53,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print L_train.get_key(558)\n",
    "print gen_model.w[558]\n",
    "print L_train[18,558]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct   = 0\n",
    "predicted = 0\n",
    "total     = 0\n",
    "for i in range(N):\n",
    "    if L_gold_train[i,0] > 0:\n",
    "        total += 1\n",
    "    \n",
    "    if yp[i] > 0:\n",
    "        predicted += 1\n",
    "        \n",
    "        max_w = 0.0\n",
    "        cid   = -1\n",
    "        for j in L_train.getrow(i).nonzero()[1]:\n",
    "            if gen_model.w[j] > max_w:\n",
    "                max_w = gen_model.w[j]\n",
    "                cid   = L_train[i,j]\n",
    "        \n",
    "        if cid == L_gold_train[i,0]:\n",
    "            correct += 1\n",
    "\n",
    "prec   = correct / float(predicted)\n",
    "recall = correct / float(total)\n",
    "\n",
    "print \"P:\\t\", prec\n",
    "print \"R:\\t\", recall\n",
    "print \"F1:\\t\", (2*prec*recall) / (prec+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct   = 0\n",
    "predicted = 0\n",
    "total     = 0\n",
    "for i in range(N):\n",
    "    if L_gold_train[i,0] > 0:\n",
    "        total += 1\n",
    "        \n",
    "    max_w = 0.0\n",
    "    cid   = -1\n",
    "    for j in L_train.getrow(i).nonzero()[1]:\n",
    "        if gen_model.w[j] > max_w:\n",
    "            max_w = gen_model.w[j]\n",
    "            cid   = L_train[i,j]\n",
    "        \n",
    "    if cid > 0:\n",
    "        predicted += 1  \n",
    "        if cid == L_gold_train[i,0]:\n",
    "            correct += 1\n",
    "\n",
    "prec   = correct / float(predicted)\n",
    "recall = correct / float(total)\n",
    "\n",
    "print \"P:\\t\", prec\n",
    "print \"R:\\t\", recall\n",
    "print \"F1:\\t\", (2*prec*recall) / (prec+recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different cut levels:\n",
    "\n",
    "Note: G = gen model on training set, D = disc. model on test set\n",
    "\n",
    "* Pos: 1, Neg: 1, Pos-cosine: 1, Neg-cosine: 1, Thresh-cosine: 0.75 = 56 F1 G / 63 F1 D\n",
    "* TODO...\n",
    "* Pos: 3, Neg: 3, Pos-cosine: 3, Neg-cosine: 3, Thresh-cosine: 0.75 = 61 F1 G / 68 F1 D\n",
    "* Pos: 3, Neg: 3, Pos-cosine: 3, Neg-cosine: 3, Thresh-cosine: [0.5, 0.75] = 63 F1 G / 65 F1 D\n",
    "* Pos: 4, Neg: 4, Pos-cosine: 4, Neg-cosine: 4, Thresh-cosine: 0.75 = 60 F1 G / 64 F1 D\n",
    "\n",
    "### Adding in drop_JJs + NEG LFs:\n",
    "\n",
    "* Pos: 2, Neg: 2, Pos-cosine: 2, Neg-cosine: 2, Thresh-cosine: 0.75 = 69 F1 G / 71 F1 D\n",
    "* Pos: 3, Neg: 3, Pos-cosine: 3, Neg-cosine: 3, Thresh-cosine: 0.75 = 70 F1 G / 73 F1 D\n",
    "\n",
    "\n",
    "#### Note: we're not yet dealing with acronyms!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hf = np.ones(L_train_b.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(L_train_b.shape[0]):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train_b.getrow(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train_b.sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.utils import sparse_abs\n",
    "sparse_abs(L_train_b).sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hf = np.where(L_train_b.sum(1) == sparse_abs(L_train_b).sum(1), np.sign(L_train_b.sum(1)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_gold_train_b = np.sign(L_gold_train.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_gold_train_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_gold_train_b.T.dot(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hf.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_train_b_sum = L_train_b.sum(1)\n",
    "L_train_b_abs_sum = sparse_abs(L_train_b).sum(1)\n",
    "L_train_b_sum_abs = sparse_abs(L_train_b.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "wrong   = 0\n",
    "for i in range(L_train_b.shape[0]):\n",
    "    if L_train_b_sum[i] < 0 and L_train_b_sum_abs[i] == L_train_b_abs_sum[i]:\n",
    "        if L_gold_train[i] < 0:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "            \n",
    "print correct\n",
    "print wrong\n",
    "print correct / float(correct + wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = L_train.shape[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = 0\n",
    "correct  = 0\n",
    "for i,c in enumerate(train):\n",
    "    if i % 1000 == 0:\n",
    "        if filtered > 0:\n",
    "            print i, filtered, correct, correct / float(filtered)\n",
    "        else:\n",
    "            print i, filtered, correct\n",
    "    \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for lf_name, label in LFG_MESH_exact(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    for lf_name, label in LFG_MESH_exact_drop_JJs(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    #if neg > 0 and pos == 0:\n",
    "    if neg > pos:\n",
    "        filtered += 1\n",
    "        if L_gold_train[i] < 0:\n",
    "            correct += 1\n",
    "\n",
    "print filtered\n",
    "print correct\n",
    "print correct / float(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = 0\n",
    "correct  = 0\n",
    "for i,c in enumerate(train):\n",
    "    if i % 1000 == 0:\n",
    "        if filtered > 0:\n",
    "            print i, filtered, correct, correct / float(filtered)\n",
    "        else:\n",
    "            print i, filtered, correct\n",
    "    \n",
    "    if len(c.disease.get_attrib_tokens()) == 1 and c.disease.get_span().lower() not in cd.term_to_sids:\n",
    "        filtered += 1\n",
    "        if L_gold_train[i] < 0:\n",
    "            correct += 1\n",
    "\n",
    "print filtered\n",
    "print correct\n",
    "print correct / float(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LF STATS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lfs = L_train.lf_stats(labels=L_gold_train, est_accs=odds_to_prob(gen_model.w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BAD_LFs = [578, 627, 603, 687, 573, 579]\n",
    "for i in BAD_LFs:\n",
    "    gen_model.w[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lfs.nsmallest(50, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for lf in lfs.iterrows():\n",
    "    lf_name, s = lf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "N_dev = L_gold_dev.shape[0]\n",
    "\n",
    "fps = []\n",
    "fns = []\n",
    "for i in range(N_dev):\n",
    "    if yp[i] > 0 and L_gold_dev[i] < 0:\n",
    "        fps.append(i)\n",
    "    elif yp[i] < 0 and L_gold_dev[i] > 0:\n",
    "        fns.append(i)\n",
    "\n",
    "shuffle(fps)\n",
    "shuffle(fns)\n",
    "\n",
    "print len(fps)\n",
    "print len(fns)\n",
    "\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "fn_cands = [F_dev.get_candidate(i) for i in fns[:100]]\n",
    "svn      = SentenceNgramViewer(fn_cands, session)\n",
    "svn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exact_match = 0\n",
    "for i in fns:\n",
    "    c = F_dev.get_candidate(i)\n",
    "    if c.disease.get_span() in mesh_tree:\n",
    "        exact_match += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = svn.get_selected()\n",
    "\n",
    "mesh_tree[c.disease.get_span()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.disease.get_attrib_tokens('pos_tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mesh_tree['alcohol abuse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Label\n",
    "\n",
    "l = session.query(Label).filter(Label.candidate == c).one()\n",
    "CID_to_MESH[l.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = F_dev.get_row_index(c)\n",
    "[(F_dev.get_key(k), disc_model.w[k]) for k in F_dev.getrow(i).nonzero()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_dev.get_key(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why is Parkinson's disease not caught?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "fp_cands = [F_dev.get_candidate(i) for i in fps[:100]]\n",
    "sv       = SentenceNgramViewer(fp_cands, session)\n",
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEG_PHRASES = [\n",
    "    'stenosis',\n",
    "    'further attention',\n",
    "    'presence',\n",
    "    'absence',\n",
    "    'syndrome',\n",
    "    'association',\n",
    "    'strain',\n",
    "    'progression'\n",
    "]\n",
    "\n",
    "NEG_END_WORDS = [\n",
    "    'therapies',\n",
    "    'muscles',\n",
    "    'concentrations',\n",
    "    'normal',\n",
    "    'heart',\n",
    "    'side',\n",
    "    'sinus',\n",
    "    'convulsants',\n",
    "    'latencies',\n",
    "    'findings',\n",
    "    'doses',\n",
    "    'remission'\n",
    "]\n",
    "\n",
    "def end_in_plural(c):\n",
    "    pass\n",
    "\n",
    "def body_part(c):\n",
    "    pass\n",
    "\n",
    "def not_exact_single_word(d):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
