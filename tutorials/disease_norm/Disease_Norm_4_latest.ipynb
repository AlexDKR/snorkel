{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Norm\n",
    "\n",
    "In this example, we'll be writing an application to extract *mentions of* diseases from Pubmed abstracts, using annotations from the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  This tutorial, which has 5 parts, walks through the process of constructing a model to classify _candidate_ disease mentions as either true (i.e., that it is truly a mention of a disease) or false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Candidates + Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Disease = candidate_subclass('Disease', ['disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28087\n",
      "27896\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "train = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Training Candidates').one()\n",
    "print len(train)\n",
    "dev = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Development Candidates').one()\n",
    "print len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28087, 1)\n",
      "(27896, 1)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "\n",
    "label_manager = LabelManager()\n",
    "\n",
    "L_gold_train = label_manager.load(session, train, \"CDR Training Label Set\")\n",
    "print L_gold_train.shape\n",
    "L_gold_dev = label_manager.load(session, dev, \"CDR Development Label Set\")\n",
    "print L_gold_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process / Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureManager\n",
    "\n",
    "feature_manager = FeatureManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time F_train = feature_manager.create(session, train, 'Train Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time F_dev = feature_manager.update(session, dev, 'Train Features', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, load if already processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time F_train = feature_manager.load(session, train, 'Train Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time F_dev = feature_manager.load(session, dev, 'Train Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MESH ID -> CID mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cPickle import load\n",
    "MESH_to_CID = load(open('MESH_to_CID.pkl', 'rb'))\n",
    "diseases    = load(open('diseases.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mesh_to_terms = defaultdict(set)\n",
    "for term, mid in diseases.iteritems():\n",
    "    mesh_to_terms[mid].add(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a canonical dictionary (CD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import CanonicalDictionary\n",
    "cd = CanonicalDictionary(MESH_to_CID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MESH to CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27885 entries\n"
     ]
    }
   ],
   "source": [
    "# Load MESH\n",
    "from utils import load_mesh_raw\n",
    "mesh_entries = load_mesh_raw('data/desc2016.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151006"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add MESH to cd\n",
    "for entry in mesh_entries:\n",
    "    mid, ps, terms = entry\n",
    "    paths = [[p[0]] + p[1:].split('.') for p in ps]\n",
    "    for term in terms:\n",
    "        cd.add_term(term, mid, tree_paths=paths)\n",
    "\n",
    "len(cd.term_to_sids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MEDIC to CD\n",
    "\n",
    "Custom CTD diseases dictionary made from MESH category C + OMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11864 MEDIC entries\n"
     ]
    }
   ],
   "source": [
    "from utils import load_MEDIC, load_mesh_raw\n",
    "medic_entries, MEDIC_to_CID = load_MEDIC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add MESH to cd\n",
    "for entry in medic_entries:\n",
    "    if entry.id.startswith(\"MESH\"):\n",
    "        mid = entry.id.split(\":\")[1]\n",
    "    elif len(entry.parent_ids) > 0 and entry.parent_ids[0].startswith(\"MESH\"):\n",
    "        mid = entry.parent_ids[0].split(\":\")[1]\n",
    "    else:\n",
    "        raise KeyError(entry)\n",
    "    \n",
    "    paths = []\n",
    "    for p in entry.tree_nums:\n",
    "        x = p.split(\"/\")[0]\n",
    "        paths.append([x[0]] + x[1:].split('.'))\n",
    "    \n",
    "    terms = [entry.name] + entry.synonyms\n",
    "    for term in terms:\n",
    "        cd.add_term(term, mid, tree_paths=paths)\n",
    "        \n",
    "len(cd.term_to_sids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add UMLS to CD\n",
    "\n",
    "This may or may not be all of the UMLS... file from Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801566"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('cui2mesh.tsv', 'rb') as f:\n",
    "    for line in f:\n",
    "        term, cui, mid = line.rstrip('\\n').split('\\t')\n",
    "        cd.add_term(term, mid)\n",
    "\n",
    "len(cd.term_to_sids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cPickle import dump\n",
    "dump(cd, open('cd.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Add MESH supp?  No paths though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing some multinomial LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Beware of LF rollback bug!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TYPE I LF: Subsets of MESH dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEEN_GLOBAL = defaultdict(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESH exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POS_DEPTH = 3\n",
    "NEG_DEPTH = 3\n",
    "def LFG_CD_match(c, p, key_mod=None, seen_global=None, max_paths_per_sid=1):\n",
    "    \"\"\"\n",
    "    Given a candidate c, some transformed candidate disease phrase p,\n",
    "    and an optional key name modifier key_mod to be appended, return a generator\n",
    "    of key, value pairs\n",
    "    \"\"\"\n",
    "    if p in cd.term_to_sids:\n",
    "        for sid in cd.term_to_sids[p]:\n",
    "            cid   = cd.sid_to_cid[sid] if sid in cd.sid_to_cid else -1\n",
    "            paths = cd.tree_paths[sid]\n",
    "            for path in paths[:max_paths_per_sid]:\n",
    "                    \n",
    "                # NOTE: path may be shorter than max depth if higher up in the tree (e.g. 'cancer', 'ischemia')!\n",
    "                key = \"-\".join(path[:POS_DEPTH]) if cid > 0 else \"-\".join(path[:NEG_DEPTH])\n",
    "                        \n",
    "                # CHECK AGAINST / ADD TO GLOBAL SEEN SET!\n",
    "                # To make sure that relaxations of an LF don't overlap with each other on any given candidate\n",
    "                if seen_global is None or c.id not in seen_global[key]:\n",
    "                    if seen_global is not None:\n",
    "                        seen_global[key].add(c.id)\n",
    "                    if key_mod:\n",
    "                        key += \"-\" + key_mod\n",
    "                    yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LFG_MESH_exact(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return LFG_CD_match(c, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train_1 = label_manager.create(session, train, 'LF Training Labels 1', f=LFG_MESH_exact)\n",
    "L_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_dev_1 = label_manager.create(session, dev, 'LF Development Labels 1', f=LFG_MESH_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or load if already computed:\n",
    "%time L_train_1 = label_manager.load(session, train, 'LF Training Labels 1')\n",
    "L_train_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop JJs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MOD_RGX = r'JJ.*|VB.*|RB.*'\n",
    "\n",
    "def LFG_drop_leading_modifiers(c):\n",
    "    words    = c.disease.get_attrib_tokens()\n",
    "    pos_tags = c.disease.get_attrib_tokens('pos_tags')\n",
    "    while re.match(MOD_RGX, pos_tags[0]):\n",
    "        words    = words[1:]\n",
    "        pos_tags = pos_tags[1:]\n",
    "        p   = \" \".join(words)\n",
    "        \n",
    "        # Hackey, but works for now...\n",
    "        g = LFG_CD_match(c, p, key_mod=\"DJ\")\n",
    "        if len(list(g)) > 0:\n",
    "            for key, cid in LFG_CD_match(c, p, key_mod=\"DJ\"):\n",
    "                yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train_2 = label_manager.create(session, train, 'LF Training Labels 2', LFG_drop_leading_modifiers)\n",
    "L_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_dev_2 = label_manager.create(session, dev, 'LF Development Labels 2', LFG_drop_leading_modifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or load if already computed:\n",
    "%time L_train_2 = label_manager.load(session, train, 'LF Training Labels 2')\n",
    "L_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REMOVE_COMMON = r'.*induced|patient.*|drug|inhibitor|\\d+|human|mouse|mice|rats?|with|syndrome|famil.*|s$|low(er)?|upper|left|right|top|bottom|subjects?'\n",
    "def remove_common(c):\n",
    "    p = re.sub(r'\\s\\s+', ' ', re.sub(REMOVE_COMMON, '', c.disease.get_span().lower())).strip()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LFG_MESH_exact_remove_common(c):\n",
    "    p = remove_common(c)\n",
    "    return LFG_CD_match(c, p, key_mod=\"RC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train_3 = label_manager.create(session, train, 'LF Training Labels 3', LFG_MESH_exact_remove_common)\n",
    "L_train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_dev_3 = label_manager.create(session, dev, 'LF Development Labels 3.1', LFG_MESH_exact_remove_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or load if already computed:\n",
    "%time L_train_3 = label_manager.load(session, train, 'LF Training Labels 3')\n",
    "L_train_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESH TF-IDF cosine match: POSITIVE terms\n",
    "\n",
    "Only cosine match with _positive_ (i.e. C, F03 MESH disease terms) here!\n",
    "\n",
    "**Note: we want to avoid positive reinforcement amongst these, so track seen / only emit one:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEEN_COSINE_POS = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from entity_norm import CanonDictVectorizer \n",
    "\n",
    "# Create a vectorizer based around this \n",
    "cd_vectorizer = CanonDictVectorizer(cd.term_to_sids, other_phrases=[])\n",
    "\n",
    "# Vectorize the dictionary\n",
    "D_pos   = cd_vectorizer.vectorize_phrases(cd.pos_terms)\n",
    "D_pos_t = D_pos.T\n",
    "D_pos_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "POS_DEPTH = 3\n",
    "NEG_DEPTH = 3\n",
    "THRESH    = 0.75\n",
    "def LFG_CD_cosine_match(c, p, vectorizer, D_t, terms, thresh=THRESH, seen_global=None, max_paths_per_sid=1, key_mod=None):\n",
    "    cx = vectorizer.vectorize_phrases([p])\n",
    "    m  = cx * D_t\n",
    "    m  = m.tocoo()\n",
    "\n",
    "    best_match = defaultdict(lambda : (0, None))\n",
    "    for i, s in enumerate(m.data):\n",
    "        if s > thresh:\n",
    "            j = m.col[i]\n",
    "            t = terms[j]\n",
    "            for sid in cd.term_to_sids[t]:\n",
    "                cid = cd.sid_to_cid[sid] if sid in cd.sid_to_cid else -1\n",
    "                for path in cd.tree_paths[sid][:max_paths_per_sid]:\n",
    "                    key  = \"-\".join(path[:POS_DEPTH]) if cid > 0 else \"-\".join(path[:NEG_DEPTH])\n",
    "                    if s > best_match[key][0]:\n",
    "                        best_match[key] = (s, cid)\n",
    "\n",
    "    for key, x in best_match.iteritems():\n",
    "        s, cid = x\n",
    "        \n",
    "        # CHECK AGAINST / ADD TO GLOBAL SEEN SET!\n",
    "        # To make sure that relaxations of an LF don't overlap with each other on any given candidate\n",
    "        if seen_global is None or c.id not in seen_global[key]:\n",
    "            if seen_global is not None:\n",
    "                seen_global[key].add(c.id)\n",
    "            key += \"-c\"\n",
    "            if key_mod:\n",
    "                key += key_mod\n",
    "            yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LFG_CD_cosine_match_pos(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return LFG_CD_cosine_match(c,p, cd_vectorizer, D_pos_t, cd.pos_terms, seen_global=SEEN_COSINE_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train_4 = label_manager.create(session, train, 'LF Training Labels 4.1', LFG_CD_cosine_match_pos)\n",
    "L_train_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_dev_4 = label_manager.create(session, dev, 'LF Development Labels 4', LFG_CD_cosine_match_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or load if already computed:\n",
    "%time L_train_4 = label_manager.load(session, train, 'LF Training Labels 4')\n",
    "L_train_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform -> pos. cosine match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LFG_CD_cosine_match_DJ(c):\n",
    "    words    = c.disease.get_attrib_tokens()\n",
    "    pos_tags = c.disease.get_attrib_tokens('pos_tags')\n",
    "    while re.match(MOD_RGX, pos_tags[0]):\n",
    "        words    = words[1:]\n",
    "        pos_tags = pos_tags[1:]\n",
    "        p   = \" \".join(words)\n",
    "        \n",
    "        # Very hackey, but works for now...\n",
    "        g = LFG_CD_cosine_match(c, p, cd_vectorizer, D_pos_t, cd.pos_terms, key_mod=\"-DJ\", seen_global=SEEN_COSINE_POS)\n",
    "        if len(list(g)) > 0:\n",
    "            for key, cid in LFG_CD_cosine_match(c, p, cd_vectorizer, D_pos_t, cd.pos_terms, key_mod=\"-DJ\", seen_global=SEEN_COSINE_POS):\n",
    "                yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train_4_T1 = label_manager.create(session, train, 'LF Training Labels 4 T1', LFG_CD_cosine_match_DJ)\n",
    "L_train_4_T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_train_4_T1 = label_manager.create(session, train, 'LF Training Labels 4 T1', LFG_CD_cosine_match_DJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_dev_4_T1 = label_manager.create(session, dev, 'LF Development Labels 4 T1', LFG_CD_cosine_match_DJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LFG_CD_cosine_match_RC(c):\n",
    "    p = remove_common(c)\n",
    "    if p != c.disease.get_span().lower():\n",
    "        for key, cid in LFG_CD_cosine_match(c, p, cd_vectorizer, D_pos_t, cd.pos_terms, key_mod=\"-RC\", seen_global=SEEN_COSINE_POS):\n",
    "            yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train_4_T2 = label_manager.create(session, train, 'LF Training Labels 4 T2', LFG_CD_cosine_match_RC)\n",
    "L_train_4_T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_dev_4_T2 = label_manager.create(session, dev, 'LF Development Labels 4 T2', LFG_CD_cosine_match_RC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neg cosine match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize the dictionary\n",
    "D_neg   = cd_vectorizer.vectorize_phrases(cd.neg_terms)\n",
    "D_neg_t = D_neg.T\n",
    "D_neg_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LFG_CD_cosine_match_neg(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return LFG_CD_cosine_match(c, p, cd_vectorizer, D_neg_t, cd.neg_terms, thresh=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train_4_N = label_manager.create(session, train, 'LF Training Labels 4 N', LFG_CD_cosine_match_neg)\n",
    "L_train_4_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_dev_4_N = label_manager.create(session, dev, 'LF Development Labels 4 N', LFG_CD_cosine_match_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting in some negative LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from lf_terms import *\n",
    "from snorkel.lf_helpers import get_left_tokens, get_right_tokens\n",
    "from utils import *\n",
    "from Disease_Tagging_Tutorial_LFs import *\n",
    "chemicals = load_chemdner_dictionary()\n",
    "\n",
    "def LF_organs(c):\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens()).lower()\n",
    "    return -1 if phrase in organs else 0      \n",
    "\n",
    "def LF_chemical_name(c):\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens())\n",
    "    return -1 if phrase in chemicals and not phrase.isupper() and phrase.lower() not in cd.term_to_sids else 0\n",
    "\n",
    "def LF_bodypart(c):\n",
    "    phrase = re.sub(r's$', '', \" \".join(c[0].get_attrib_tokens()).lower())\n",
    "    return -1 if phrase in bodypart else 0  \n",
    "\n",
    "def LF_protein_chemical_abbrv(c):\n",
    "    '''Gene/protein/chemical name'''\n",
    "    lemma = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return -1 if re.search(\"\\d+\",lemma) else 0\n",
    "\n",
    "def LF_base_pair_seq(c): \n",
    "    lemma = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return -1 if re.search(\"^[GACT]{2,}$\",lemma) else 0\n",
    "\n",
    "LFs_false = [LF_chemical_name,\n",
    "             LF_organs,\n",
    "             LF_bodypart,\n",
    "             LF_protein_chemical_abbrv,\n",
    "             LF_base_pair_seq,\n",
    "             #LF_too_vague,\n",
    "             #LF_neg_surfix,\n",
    "             LF_non_common_disease,\n",
    "             LF_non_disease_acronyms,\n",
    "             #LF_pos_in,\n",
    "             LF_gene_chromosome_link,\n",
    "             LF_right_window_incomplete,\n",
    "             #LF_negative_indicator\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train_5 = label_manager.create(session, train, 'LF Training Labels 5', LFs_false)\n",
    "L_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_dev_5 = label_manager.create(session, dev, 'LF Development Labels 5', LFs_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or load if already computed:\n",
    "%time L_train_5 = label_manager.load(session, train, 'LF Training Labels 5.1')\n",
    "L_train_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More neg. LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NEG_COMMON_RGX = r'(finding|disease|syndrome|marker|defecit|.*event|mean|median|mg)s?'\n",
    "def LF_common_neg_phrases(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return -1 if re.match(NEG_COMMON_RGX, p) else 0\n",
    "\n",
    "NEG_AFTER_WORDS = frozenset(['of', 'to'])\n",
    "def LF_neg_after(c):\n",
    "    rw = get_right_tokens(c, window=1, attrib='lemmas')\n",
    "    return -1 if len(rw) > 0 and rw[0] in NEG_AFTER_WORDS and c.disease.get_span().lower() not in cd.term_to_sids else 0\n",
    "\n",
    "def LF_after_num(c):\n",
    "    lw = get_left_tokens(c, window=1, attrib='lemmas')\n",
    "    return -1 if len(lw) > 0 and re.match(r'\\d+', lw[0]) else 0\n",
    "\n",
    "def LF_too_short(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return -1 if len(p) < 3 else 0\n",
    "\n",
    "BAD_ENDINGS_RGX = r'(type|trait|cell)s?$'\n",
    "def LF_bad_endings(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return -1 if re.search(BAD_ENDINGS_RGX, p) else 0\n",
    "\n",
    "BAD_MESH_TERMS = frozenset(['disease', 'diseases', 'conversion'])\n",
    "def LF_bad_MESH_entries(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return -1 if p in BAD_MESH_TERMS else 0\n",
    "\n",
    "LFs_false_2 = [\n",
    "    LF_common_neg_phrases,\n",
    "    LF_neg_after,\n",
    "    LF_after_num,\n",
    "    LF_too_short,\n",
    "    LF_bad_endings,\n",
    "    LF_bad_MESH_entries\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train_6 = label_manager.create(session, train, 'LF Training Labels 6', LFs_false_2)\n",
    "L_train_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_dev_6 = label_manager.create(session, dev, 'LF Development Labels 6', LFs_false_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running gen. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import binarize_LF_matrix, get_binarized_score\n",
    "from snorkel.annotations import merge_annotations\n",
    "\n",
    "L_train = merge_annotations([\n",
    "        L_train_1\n",
    "        , L_train_2\n",
    "        , L_train_3\n",
    "        , L_train_4\n",
    "        #, L_train_4_T1\n",
    "        , L_train_4_T2\n",
    "        , L_train_4_N\n",
    "        , L_train_5\n",
    "        , L_train_6\n",
    "    ])\n",
    "\n",
    "L_train_b = binarize_LF_matrix(L_train)\n",
    "L_train_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model_train = NaiveBayes()\n",
    "%time gen_model_train.train(L_train_b, n_iter=10000, rate=1e-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp_gt_train = gen_model_train.predict(L_train_b, b=0.5)\n",
    "get_binarized_score(yp_gt_train, L_gold_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print LF stats...\n",
    "from snorkel.learning import odds_to_prob\n",
    "lfs = L_train.lf_stats(labels=L_gold_train, est_accs=odds_to_prob(gen_model.w))\n",
    "lfs.nlargest(50, \"coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis\n",
    "_F1=63_\n",
    "\n",
    "ROUND 1:\n",
    "* _DONE: Take out 'F' category? [6]_\n",
    "* _DONE: Take out supplemental entries [2]_\n",
    "* _DONE: Don't emit all tree paths?  **This essentially gives many more votes if it has multiple tree paths!!!** [3]_\n",
    "\n",
    "**_--> F1=69_**\n",
    "\n",
    "ROUND 2:\n",
    "* _DONE: Take out LF-negative-indicator: no longer appropriate here! [1]_\n",
    "* _DONE: Common transforms [3]_\n",
    "* _DONE: Remove leading VB(N|D) also! [2]_\n",
    "* _DONE: Iterate over all JJs removed; stop at first (longest) match [2]_\n",
    "* _DONE: Try \"no reinforcing deps\" setting where we avoid the positive reinforcement of cascading overlap?_\n",
    "\n",
    "**_--> F1=72_**\n",
    "\n",
    "ROUND 3:\n",
    "* _DONE: Negative cosine matches tend to have super-high weight... **take these out??**_\n",
    "    - _**In particular, we match e.g. the disease + associated proteins, body parts, etc...**_\n",
    "    - _We are missing some **exact matches**...!!!  Just override model here?_\n",
    "    - _Or, add multiple threshs again for cosine match (e.g. milk fever case...)?_\n",
    "* _DONE: Weird words in chemicals dictionary (\"hepatitis\", \"leprosy\")? Take chemicals - diseases instead? [2]_\n",
    "\n",
    "**_--> F1=73_**\n",
    "\n",
    "ROUND 4:\n",
    "* _DONE: `LF-neg-after` is broken! [2]_\n",
    "* _DONE: Try to avoid `LF-after-num` errors by checking for exact match [2]_\n",
    "* _DONE: Try running transforms through vector matcher as well?_\n",
    "\n",
    "**_--> F1=74_**\n",
    "\n",
    "ROUND 5:\n",
    "* Positive reinforcement between vector transform matches seems to be an issue... [2]\n",
    "* Check for the word before \"X (Y)\" [1]\n",
    "\n",
    "\n",
    "#### Other ideas:\n",
    "* \"C-induced D\", \"C D\": explicitly create LF for this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "N_train = L_gold_train.shape[0]\n",
    "\n",
    "fps    = []\n",
    "fns    = []\n",
    "fns_na = []\n",
    "for i in range(N_train):\n",
    "    if yp[i] > 0 and L_gold_train[i] < 0:\n",
    "        fps.append(i)\n",
    "    elif yp[i] <= 0 and L_gold_train[i] > 0:\n",
    "        if yp[i] == 0:\n",
    "            fns_na.append(i)\n",
    "        else:\n",
    "            fns.append(i)\n",
    "\n",
    "shuffle(fps)\n",
    "shuffle(fns)\n",
    "shuffle(fns_na)\n",
    "\n",
    "print len(fps)\n",
    "print len(fns)\n",
    "print len(fns_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "fp_cands = [L_train.get_candidate(i) for i in fps[:100]]\n",
    "svp      = SentenceNgramViewer(fp_cands, session)\n",
    "svp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = svp.get_selected()\n",
    "print c\n",
    "print c.disease.get_attrib_tokens('pos_tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "[stemmer.stem(w) for w in c.disease.get_attrib_tokens()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Label\n",
    "session.query(Label).filter(Label.candidate == c).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.gen_learning import odds_to_prob\n",
    "i = L_train.get_row_index(c)\n",
    "\n",
    "for j in L_train.getrow(i).nonzero()[1]:\n",
    "    print L_train.get_key(j), odds_to_prob(gen_model.w[j]), int(L_train[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mesh_to_terms[cd.cid_to_sid[755]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "THRESH            = 0.75\n",
    "def vector_matches(p, Dt, vectorizer, thresh=THRESH):\n",
    "    cx  = cd_vectorizer.vectorize_phrases([p])\n",
    "    m   = cx * Dt\n",
    "    m   = m.tocoo()\n",
    "    out = []\n",
    "    for i, s in enumerate(m.data):\n",
    "        if s > THRESH:\n",
    "            j    = m.col[i]\n",
    "            t    = cd.pos_terms[j]\n",
    "            sids = cd.term_to_sids[t]\n",
    "            out.append((s, j, t, sids))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find most common words\n",
    "word_fs = [(len(cids), word) for word, cids in cd_vectorizer.word_to_cids.iteritems()]\n",
    "word_fs.sort(key=lambda x : -x[0])\n",
    "word_fs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running gen. model: Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import binarize_LF_matrix, get_binarized_score\n",
    "from snorkel.annotations import merge_annotations\n",
    "\n",
    "L_dev = merge_annotations([\n",
    "        L_dev_1\n",
    "        , L_dev_2\n",
    "        , L_dev_3\n",
    "        , L_dev_4\n",
    "        #, L_dev_4_T1\n",
    "        , L_dev_4_T2\n",
    "        , L_dev_4_N\n",
    "        , L_dev_5\n",
    "        , L_dev_6\n",
    "    ])\n",
    "\n",
    "L_dev_b = binarize_LF_matrix(L_dev)\n",
    "L_dev_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model_dev = NaiveBayes()\n",
    "%time gen_model_dev.train(L_dev_b, n_iter=10000, rate=1e-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp_gd_dev = gen_model_dev.predict(L_dev_b, b=0.5)\n",
    "get_binarized_score(yp_gd_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Discriminative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import LogReg\n",
    "\n",
    "train_marginals = gen_model_train.marginals(L_train_b)\n",
    "\n",
    "disc_model = LogReg()\n",
    "disc_model.train(F_train, train_marginals, n_iter=2000, rate=1e-3, mu=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp_d_train = disc_model.predict(F_train)\n",
    "get_binarized_score(yp_d_train, L_gold_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp_d_dev = disc_model.predict(F_dev, b=0.5)\n",
    "get_binarized_score(yp_d_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly-supervised ET baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_model = LogReg()\n",
    "\n",
    "gold_marginals_train = np.array([1.0 if L_gold_train[i,0] > 0.0 else 0.0 for i in range(L_gold_train.shape[0])])\n",
    "\n",
    "disc_model.train(F_train, gold_marginals_train, n_iter=2000, rate=1e-3, mu=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp = disc_model.predict(F_train)\n",
    "get_binarized_score(yp, L_gold_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp = disc_model.predict(F_dev, b=0.5)\n",
    "get_binarized_score(yp, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_gold_dev[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supervised_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(disc_model.marginals(F_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.gen_learning import odds_to_prob\n",
    "plt.hist(odds_to_prob(gen_model.w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train.lf_stats(labels=L_gold_train, est_accs=odds_to_prob(gen_model.w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try overriding with any exact matches...\n",
    "yp_d_dev_um = np.zeros(L_gold_dev.shape[0])\n",
    "for i,c in enumerate(dev):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for lf_name, label in LFG_MESH_exact(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    for lf_name, label in LFG_drop_leading_modifiers(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    for lf_name, label in LFG_MESH_exact_remove_common(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    if neg > 0 and pos == 0:\n",
    "        yp_d_dev_um[i] = -1\n",
    "    elif pos > 0 and neg == 0:\n",
    "        yp_d_dev_um[i] = 1\n",
    "    else:\n",
    "        yp_d_dev_um[i] = yp_d_dev[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_binarized_score(yp_d_dev_um, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact match baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try overriding with any exact matches...\n",
    "yp_em = np.zeros(L_gold_dev.shape[0])\n",
    "for i,c in enumerate(dev):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for lf_name, label in LFG_MESH_exact(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    if neg > 0:\n",
    "        yp_em[i] = -1\n",
    "    elif pos > 0:\n",
    "        yp_em[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_binarized_score(yp_em, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, getting CIDs in simple heuristic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = L_gold_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp_idxs = [i for i in range(N) if yp[i] == 1 and L_gold_train[i] > 0]\n",
    "pp_idxs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 18\n",
    "for j in L_train.getrow(i).nonzero()[1]:\n",
    "    print L_train.get_key(j), gen_model.w[j], L_train[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 19\n",
    "for j in L_train.getrow(i).nonzero()[1]:\n",
    "    print L_train.get_key(j), gen_model.w[j], L_train[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_gold_train[19,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print L_train.get_key(558)\n",
    "print gen_model.w[558]\n",
    "print L_train[18,558]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "Take the weighted-sum max (i.e. if same CID appears multiple times, add them up!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N         = L_gold_dev.shape[0]\n",
    "correct   = 0\n",
    "predicted = 0\n",
    "total     = 0\n",
    "for i in range(N):\n",
    "    if L_gold_dev[i,0] > 0:\n",
    "        total += 1\n",
    "    \n",
    "    if yp_d_dev_um[i] > 0:\n",
    "        predicted += 1\n",
    "        \n",
    "        scored = defaultdict(float)\n",
    "        for j in L_dev.getrow(i).nonzero()[1]:\n",
    "            scored[L_dev[i,j]] += gen_model_dev.w[j]\n",
    "        scores = list(scored.iteritems())\n",
    "        if len(scores) > 0:\n",
    "            scores.sort(key=lambda x : -x[1])\n",
    "            cid = scores[0][0]\n",
    "        else:\n",
    "            cid = -1\n",
    "        \n",
    "        if cid == L_gold_dev[i,0]:\n",
    "            correct += 1\n",
    "\n",
    "prec   = correct / float(predicted)\n",
    "recall = correct / float(total)\n",
    "\n",
    "print \"P:\\t\", prec\n",
    "print \"R:\\t\", recall\n",
    "print \"F1:\\t\", (2*prec*recall) / (prec+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N         = L_gold_dev.shape[0]\n",
    "correct   = 0\n",
    "predicted = 0\n",
    "total     = 0\n",
    "for i in range(N):\n",
    "    if L_gold_dev[i,0] > 0:\n",
    "        total += 1\n",
    "        \n",
    "    scored = defaultdict(float)\n",
    "    for j in L_dev.getrow(i).nonzero()[1]:\n",
    "        scored[L_dev[i,j]] += gen_model_dev.w[j]\n",
    "    scores = list(scored.iteritems())\n",
    "    if len(scores) > 0:\n",
    "        scores.sort(key=lambda x : -x[1])\n",
    "        cid = scores[0][0]\n",
    "    else:\n",
    "        cid = -1\n",
    "    \n",
    "    if cid > 0:\n",
    "        predicted += 1\n",
    "        if cid == L_gold_dev[i,0]:\n",
    "            correct += 1\n",
    "\n",
    "prec   = correct / float(predicted)\n",
    "recall = correct / float(total)\n",
    "\n",
    "print \"P:\\t\", prec\n",
    "print \"R:\\t\", recall\n",
    "print \"F1:\\t\", (2*prec*recall) / (prec+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N         = L_gold_dev.shape[0]\n",
    "correct   = 0\n",
    "predicted = 0\n",
    "total     = 0\n",
    "for i in range(N):\n",
    "    if L_gold_dev[i,0] > 0:\n",
    "        total += 1\n",
    "    \n",
    "    if yp_d_dev_um[i] > 0:\n",
    "        predicted += 1\n",
    "        \n",
    "        max_w = 0.0\n",
    "        cid   = -1\n",
    "        for j in L_dev.getrow(i).nonzero()[1]:\n",
    "            if gen_model_dev.w[j] > max_w:\n",
    "                max_w = gen_model_dev.w[j]\n",
    "                cid   = L_dev[i,j]\n",
    "        \n",
    "        if cid == L_gold_dev[i,0]:\n",
    "            correct += 1\n",
    "\n",
    "prec   = correct / float(predicted)\n",
    "recall = correct / float(total)\n",
    "\n",
    "print \"P:\\t\", prec\n",
    "print \"R:\\t\", recall\n",
    "print \"F1:\\t\", (2*prec*recall) / (prec+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N         = L_gold_dev.shape[0]\n",
    "correct   = 0\n",
    "predicted = 0\n",
    "total     = 0\n",
    "for i in range(N):\n",
    "    if L_gold_dev[i,0] > 0:\n",
    "        total += 1\n",
    "        \n",
    "    max_w = 0.0\n",
    "    cid   = -1\n",
    "    for j in L_dev.getrow(i).nonzero()[1]:\n",
    "        if gen_model_dev.w[j] > max_w:\n",
    "            max_w = gen_model_dev.w[j]\n",
    "            cid   = L_dev[i,j]\n",
    "    \n",
    "    if cid > 0:\n",
    "        predicted += 1\n",
    "        if cid == L_gold_dev[i,0]:\n",
    "            correct += 1\n",
    "\n",
    "prec   = correct / float(predicted)\n",
    "recall = correct / float(total)\n",
    "\n",
    "print \"P:\\t\", prec\n",
    "print \"R:\\t\", recall\n",
    "print \"F1:\\t\", (2*prec*recall) / (prec+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct   = 0\n",
    "predicted = 0\n",
    "total     = 0\n",
    "for i in range(N):\n",
    "    if L_gold_train[i,0] > 0:\n",
    "        total += 1\n",
    "        \n",
    "    max_w = 0.0\n",
    "    cid   = -1\n",
    "    for j in L_train.getrow(i).nonzero()[1]:\n",
    "        if gen_model.w[j] > max_w:\n",
    "            max_w = gen_model.w[j]\n",
    "            cid   = L_train[i,j]\n",
    "        \n",
    "    if cid > 0:\n",
    "        predicted += 1  \n",
    "        if cid == L_gold_train[i,0]:\n",
    "            correct += 1\n",
    "\n",
    "prec   = correct / float(predicted)\n",
    "recall = correct / float(total)\n",
    "\n",
    "print \"P:\\t\", prec\n",
    "print \"R:\\t\", recall\n",
    "print \"F1:\\t\", (2*prec*recall) / (prec+recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different cut levels:\n",
    "\n",
    "Note: G = gen model on training set, D = disc. model on test set\n",
    "\n",
    "* Pos: 1, Neg: 1, Pos-cosine: 1, Neg-cosine: 1, Thresh-cosine: 0.75 = 56 F1 G / 63 F1 D\n",
    "* TODO...\n",
    "* Pos: 3, Neg: 3, Pos-cosine: 3, Neg-cosine: 3, Thresh-cosine: 0.75 = 61 F1 G / 68 F1 D\n",
    "* Pos: 3, Neg: 3, Pos-cosine: 3, Neg-cosine: 3, Thresh-cosine: [0.5, 0.75] = 63 F1 G / 65 F1 D\n",
    "* Pos: 4, Neg: 4, Pos-cosine: 4, Neg-cosine: 4, Thresh-cosine: 0.75 = 60 F1 G / 64 F1 D\n",
    "\n",
    "### Adding in drop_JJs + NEG LFs:\n",
    "\n",
    "* Pos: 2, Neg: 2, Pos-cosine: 2, Neg-cosine: 2, Thresh-cosine: 0.75 = 69 F1 G / 71 F1 D\n",
    "* Pos: 3, Neg: 3, Pos-cosine: 3, Neg-cosine: 3, Thresh-cosine: 0.75 = 70 F1 G / 73 F1 D\n",
    "\n",
    "\n",
    "#### Note: we're not yet dealing with acronyms!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hf = np.ones(L_train_b.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(L_train_b.shape[0]):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train_b.getrow(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train_b.sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.utils import sparse_abs\n",
    "sparse_abs(L_train_b).sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hf = np.where(L_train_b.sum(1) == sparse_abs(L_train_b).sum(1), np.sign(L_train_b.sum(1)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_gold_train_b = np.sign(L_gold_train.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_gold_train_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_gold_train_b.T.dot(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hf.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_train_b_sum = L_train_b.sum(1)\n",
    "L_train_b_abs_sum = sparse_abs(L_train_b).sum(1)\n",
    "L_train_b_sum_abs = sparse_abs(L_train_b.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "wrong   = 0\n",
    "for i in range(L_train_b.shape[0]):\n",
    "    if L_train_b_sum[i] < 0 and L_train_b_sum_abs[i] == L_train_b_abs_sum[i]:\n",
    "        if L_gold_train[i] < 0:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "            \n",
    "print correct\n",
    "print wrong\n",
    "print correct / float(correct + wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = L_train.shape[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = 0\n",
    "correct  = 0\n",
    "for i,c in enumerate(train):\n",
    "    if i % 1000 == 0:\n",
    "        if filtered > 0:\n",
    "            print i, filtered, correct, correct / float(filtered)\n",
    "        else:\n",
    "            print i, filtered, correct\n",
    "    \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for lf_name, label in LFG_MESH_exact(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    for lf_name, label in LFG_MESH_exact_drop_JJs(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    #if neg > 0 and pos == 0:\n",
    "    if neg > pos:\n",
    "        filtered += 1\n",
    "        if L_gold_train[i] < 0:\n",
    "            correct += 1\n",
    "\n",
    "print filtered\n",
    "print correct\n",
    "print correct / float(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = 0\n",
    "correct  = 0\n",
    "for i,c in enumerate(train):\n",
    "    if i % 1000 == 0:\n",
    "        if filtered > 0:\n",
    "            print i, filtered, correct, correct / float(filtered)\n",
    "        else:\n",
    "            print i, filtered, correct\n",
    "    \n",
    "    if len(c.disease.get_attrib_tokens()) == 1 and c.disease.get_span().lower() not in cd.term_to_sids:\n",
    "        filtered += 1\n",
    "        if L_gold_train[i] < 0:\n",
    "            correct += 1\n",
    "\n",
    "print filtered\n",
    "print correct\n",
    "print correct / float(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LF STATS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lfs = L_train.lf_stats(labels=L_gold_train, est_accs=odds_to_prob(gen_model.w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BAD_LFs = [578, 627, 603, 687, 573, 579]\n",
    "for i in BAD_LFs:\n",
    "    gen_model.w[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lfs.nsmallest(50, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for lf in lfs.iterrows():\n",
    "    lf_name, s = lf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "N_dev = L_gold_dev.shape[0]\n",
    "\n",
    "fps = []\n",
    "fns = []\n",
    "for i in range(N_dev):\n",
    "    if yp[i] > 0 and L_gold_dev[i] < 0:\n",
    "        fps.append(i)\n",
    "    elif yp[i] < 0 and L_gold_dev[i] > 0:\n",
    "        fns.append(i)\n",
    "\n",
    "shuffle(fps)\n",
    "shuffle(fns)\n",
    "\n",
    "print len(fps)\n",
    "print len(fns)\n",
    "\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "fn_cands = [F_dev.get_candidate(i) for i in fns[:100]]\n",
    "svn      = SentenceNgramViewer(fn_cands, session)\n",
    "svn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exact_match = 0\n",
    "for i in fns:\n",
    "    c = F_dev.get_candidate(i)\n",
    "    if c.disease.get_span() in mesh_tree:\n",
    "        exact_match += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = svn.get_selected()\n",
    "\n",
    "mesh_tree[c.disease.get_span()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.disease.get_attrib_tokens('pos_tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mesh_tree['alcohol abuse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Label\n",
    "\n",
    "l = session.query(Label).filter(Label.candidate == c).one()\n",
    "CID_to_MESH[l.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = F_dev.get_row_index(c)\n",
    "[(F_dev.get_key(k), disc_model.w[k]) for k in F_dev.getrow(i).nonzero()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_dev.get_key(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why is Parkinson's disease not caught?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "fp_cands = [F_dev.get_candidate(i) for i in fps[:100]]\n",
    "sv       = SentenceNgramViewer(fp_cands, session)\n",
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEG_PHRASES = [\n",
    "    'stenosis',\n",
    "    'further attention',\n",
    "    'presence',\n",
    "    'absence',\n",
    "    'syndrome',\n",
    "    'association',\n",
    "    'strain',\n",
    "    'progression'\n",
    "]\n",
    "\n",
    "NEG_END_WORDS = [\n",
    "    'therapies',\n",
    "    'muscles',\n",
    "    'concentrations',\n",
    "    'normal',\n",
    "    'heart',\n",
    "    'side',\n",
    "    'sinus',\n",
    "    'convulsants',\n",
    "    'latencies',\n",
    "    'findings',\n",
    "    'doses',\n",
    "    'remission'\n",
    "]\n",
    "\n",
    "def end_in_plural(c):\n",
    "    pass\n",
    "\n",
    "def body_part(c):\n",
    "    pass\n",
    "\n",
    "def not_exact_single_word(d):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
