{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Norm\n",
    "\n",
    "In this example, we'll be writing an application to extract *mentions of* diseases from Pubmed abstracts, using annotations from the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  This tutorial, which has 5 parts, walks through the process of constructing a model to classify _candidate_ disease mentions as either true (i.e., that it is truly a mention of a disease) or false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Candidates + Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Disease = candidate_subclass('Disease', ['disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28087\n",
      "27896\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "train = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Training Candidates').one()\n",
    "print len(train)\n",
    "dev = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Development Candidates').one()\n",
    "print len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28087, 1)\n",
      "(27896, 1)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "\n",
    "label_manager = LabelManager()\n",
    "\n",
    "L_gold_train = label_manager.load(session, train, \"CDR Training Label Set\")\n",
    "print L_gold_train.shape\n",
    "L_gold_dev = label_manager.load(session, dev, \"CDR Development Label Set\")\n",
    "print L_gold_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process / Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureManager\n",
    "\n",
    "feature_manager = FeatureManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    F_train = feature_manager.load(session, train, 'Train Features')\n",
    "    F_dev   = feature_manager.load(session, dev, 'Train Features')\n",
    "except:\n",
    "    F_train = feature_manager.create(session, train, 'Train Features')\n",
    "    F_dev   = feature_manager.update(session, dev, 'Train Features', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create canonical dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MESH ID -> CID mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cPickle import load\n",
    "MESH_to_CID = load(open('MESH_to_CID.pkl', 'rb'))\n",
    "diseases    = load(open('diseases.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a canonical dictionary (CD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import CanonicalDictionary\n",
    "cd = CanonicalDictionary(MESH_to_CID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MESH to CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load MESH\n",
    "from utils import load_mesh_raw\n",
    "mesh_entries = load_mesh_raw('data/desc2016.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add MESH to cd\n",
    "for entry in mesh_entries:\n",
    "    mid, ps, terms = entry\n",
    "    paths = [[p[0]] + p[1:].split('.') for p in ps]\n",
    "    for term in terms:\n",
    "        cd.add_term(term, mid, tree_paths=paths)\n",
    "\n",
    "len(cd.term_to_sids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MEDIC to CD\n",
    "\n",
    "Custom CTD diseases dictionary made from MESH category C + OMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import load_MEDIC, load_mesh_raw\n",
    "medic_entries, MEDIC_to_CID = load_MEDIC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add MESH to cd\n",
    "for entry in medic_entries:\n",
    "    if entry.id.startswith(\"MESH\"):\n",
    "        mid = entry.id.split(\":\")[1]\n",
    "    elif len(entry.parent_ids) > 0 and entry.parent_ids[0].startswith(\"MESH\"):\n",
    "        mid = entry.parent_ids[0].split(\":\")[1]\n",
    "    else:\n",
    "        raise KeyError(entry)\n",
    "    \n",
    "    paths = []\n",
    "    for p in entry.tree_nums:\n",
    "        x = p.split(\"/\")[0]\n",
    "        paths.append([x[0]] + x[1:].split('.'))\n",
    "    \n",
    "    terms = [entry.name] + entry.synonyms\n",
    "    for term in terms:\n",
    "        cd.add_term(term, mid, tree_paths=paths)\n",
    "        \n",
    "len(cd.term_to_sids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add UMLS to CD\n",
    "\n",
    "This may or may not be all of the UMLS... file from Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('cui2mesh.tsv', 'rb') as f:\n",
    "    for line in f:\n",
    "        term, cui, mid = line.rstrip('\\n').split('\\t')\n",
    "        cd.add_term(term, mid)\n",
    "\n",
    "len(cd.term_to_sids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cPickle import dump\n",
    "dump(cd, open('cd.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing some multinomial LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Beware of LF rollback bug!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TYPE I LF: Subsets of MESH dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEEN_GLOBAL = defaultdict(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESH exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POS_DEPTH = 3\n",
    "NEG_DEPTH = 3\n",
    "def LFG_CD_match(c, p, key_mod=None, seen_global=None, max_paths_per_sid=1):\n",
    "    \"\"\"\n",
    "    Given a candidate c, some transformed candidate disease phrase p,\n",
    "    and an optional key name modifier key_mod to be appended, return a generator\n",
    "    of key, value pairs\n",
    "    \"\"\"\n",
    "    if p in cd.term_to_sids:\n",
    "        for sid in cd.term_to_sids[p]:\n",
    "            cid   = cd.sid_to_cid[sid] if sid in cd.sid_to_cid else -1\n",
    "            paths = cd.tree_paths[sid]\n",
    "            for path in paths[:max_paths_per_sid]:\n",
    "                    \n",
    "                # NOTE: path may be shorter than max depth if higher up in the tree (e.g. 'cancer', 'ischemia')!\n",
    "                key = \"-\".join(path[:POS_DEPTH]) if cid > 0 else \"-\".join(path[:NEG_DEPTH])\n",
    "                        \n",
    "                # CHECK AGAINST / ADD TO GLOBAL SEEN SET!\n",
    "                # To make sure that relaxations of an LF don't overlap with each other on any given candidate\n",
    "                if seen_global is None or c.id not in seen_global[key]:\n",
    "                    if seen_global is not None:\n",
    "                        seen_global[key].add(c.id)\n",
    "                    if key_mod:\n",
    "                        key += \"-\" + key_mod\n",
    "                    yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LFG_MESH_exact(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return LFG_CD_match(c, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    L_train_exact_match = label_manager.load(session, train, 'LF Training Labels -- Exact Match')\n",
    "except:\n",
    "    L_train_exact_match = label_manager.create(\n",
    "        session, train, 'LF Training Labels -- Exact Match', LFG_MESH_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop leading modifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MOD_RGX = r'JJ.*|VB.*|RB.*'\n",
    "\n",
    "def LFG_drop_leading_modifiers(c):\n",
    "    words    = c.disease.get_attrib_tokens()\n",
    "    pos_tags = c.disease.get_attrib_tokens('pos_tags')\n",
    "    while re.match(MOD_RGX, pos_tags[0]):\n",
    "        words    = words[1:]\n",
    "        pos_tags = pos_tags[1:]\n",
    "        p   = \" \".join(words)\n",
    "        \n",
    "        # Hackey, but works for now...\n",
    "        g = LFG_CD_match(c, p, key_mod=\"DJ\")\n",
    "        if len(list(g)) > 0:\n",
    "            for key, cid in LFG_CD_match(c, p, key_mod=\"DJ\"):\n",
    "                yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    L_train_drop_leading = label_manager.load(session, train, 'LF Training Labels -- Drop Leading')\n",
    "except:\n",
    "    L_train_drop_leading = label_manager.create(\n",
    "        session, train, 'LF Training Labels -- Drop Leading', LFG_drop_leading_modifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REMOVE_COMMON = r'.*induced|patient.*|drug|inhibitor|\\d+|human|mouse|mice|rats?|with|syndrome|famil.*|s$|low(er)?|upper|left|right|top|bottom|subjects?'\n",
    "def remove_common(c):\n",
    "    p = re.sub(r'\\s\\s+', ' ', re.sub(REMOVE_COMMON, '', c.disease.get_span().lower())).strip()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LFG_MESH_exact_remove_common(c):\n",
    "    p = remove_common(c)\n",
    "    return LFG_CD_match(c, p, key_mod=\"RC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    L_train_remove_common = label_manager.load(session, train, 'LF Training Labels -- Remove Common')\n",
    "except:\n",
    "    L_train_remove_common = label_manager.create(\n",
    "        session, train, 'LF Training Labels -- Remove Common', LFG_MESH_exact_remove_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESH TF-IDF cosine match: POSITIVE terms\n",
    "\n",
    "Only cosine match with _positive_ (i.e. C, F03 MESH disease terms) here!\n",
    "\n",
    "**Note: we want to avoid positive reinforcement amongst these, so track seen / only emit one:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEEN_COSINE_POS = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from entity_norm import CanonDictVectorizer \n",
    "\n",
    "# Create a vectorizer based around this \n",
    "cd_vectorizer = CanonDictVectorizer(cd.term_to_sids, other_phrases=[])\n",
    "\n",
    "# Vectorize the dictionary\n",
    "D_pos   = cd_vectorizer.vectorize_phrases(cd.pos_terms)\n",
    "D_pos_t = D_pos.T\n",
    "D_pos_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "POS_DEPTH = 3\n",
    "NEG_DEPTH = 3\n",
    "THRESH    = 0.75\n",
    "def LFG_CD_cosine_match(c, p, vectorizer, D_t, terms, thresh=THRESH, seen_global=None, max_paths_per_sid=1, key_mod=None):\n",
    "    cx = vectorizer.vectorize_phrases([p])\n",
    "    m  = cx * D_t\n",
    "    m  = m.tocoo()\n",
    "\n",
    "    best_match = defaultdict(lambda : (0, None))\n",
    "    for i, s in enumerate(m.data):\n",
    "        if s > thresh:\n",
    "            j = m.col[i]\n",
    "            t = terms[j]\n",
    "            for sid in cd.term_to_sids[t]:\n",
    "                cid = cd.sid_to_cid[sid] if sid in cd.sid_to_cid else -1\n",
    "                for path in cd.tree_paths[sid][:max_paths_per_sid]:\n",
    "                    key  = \"-\".join(path[:POS_DEPTH]) if cid > 0 else \"-\".join(path[:NEG_DEPTH])\n",
    "                    if s > best_match[key][0]:\n",
    "                        best_match[key] = (s, cid)\n",
    "\n",
    "    for key, x in best_match.iteritems():\n",
    "        s, cid = x\n",
    "        \n",
    "        # CHECK AGAINST / ADD TO GLOBAL SEEN SET!\n",
    "        # To make sure that relaxations of an LF don't overlap with each other on any given candidate\n",
    "        if seen_global is None or c.id not in seen_global[key]:\n",
    "            if seen_global is not None:\n",
    "                seen_global[key].add(c.id)\n",
    "            key += \"-c\"\n",
    "            if key_mod:\n",
    "                key += key_mod\n",
    "            yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LFG_CD_cosine_match_pos(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return LFG_CD_cosine_match(c,p, cd_vectorizer, D_pos_t, cd.pos_terms, seen_global=SEEN_COSINE_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    L_train_cosine_pos = label_manager.load(session, train, 'LF Training Labels -- TF-IDF Pos Terms')\n",
    "except:\n",
    "    L_train_cosine_pos = label_manager.create(\n",
    "        session, train, 'LF Training Labels -- TF-IDF Pos Terms', LFG_CD_cosine_match_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform (T) -> pos. cosine match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LFG_CD_cosine_match_RC(c):\n",
    "    p = remove_common(c)\n",
    "    if p != c.disease.get_span().lower():\n",
    "        for key, cid in LFG_CD_cosine_match(c, p, cd_vectorizer, D_pos_t, cd.pos_terms, key_mod=\"-RC\", seen_global=SEEN_COSINE_POS):\n",
    "            yield key, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    L_train_cosine_pos_T1 = label_manager.load(session, train, 'LF Training Labels -- TF-IDF Pos Terms T1')\n",
    "except:\n",
    "    L_train_cosine_pos_T1 = label_manager.create(\n",
    "        session, train, 'LF Training Labels -- TF-IDF Pos Terms T1', LFG_CD_cosine_match_RC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neg cosine match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize the dictionary\n",
    "D_neg   = cd_vectorizer.vectorize_phrases(cd.neg_terms)\n",
    "D_neg_t = D_neg.T\n",
    "D_neg_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LFG_CD_cosine_match_neg(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return LFG_CD_cosine_match(c, p, cd_vectorizer, D_neg_t, cd.neg_terms, thresh=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    L_train_cosine_neg = label_manager.load(session, train, 'LF Training Labels -- TF-IDF Neg Terms')\n",
    "except:\n",
    "    L_train_cosine_neg = label_manager.create(\n",
    "        session, train, 'LF Training Labels -- TF-IDF Neg Terms', LFG_CD_cosine_match_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting in some negative LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from lf_terms import *\n",
    "from snorkel.lf_helpers import get_left_tokens, get_right_tokens\n",
    "from utils import *\n",
    "from Disease_Tagging_Tutorial_LFs import *\n",
    "chemicals = load_chemdner_dictionary()\n",
    "\n",
    "def LF_organs(c):\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens()).lower()\n",
    "    return -1 if phrase in organs else 0      \n",
    "\n",
    "def LF_chemical_name(c):\n",
    "    phrase = \" \".join(c[0].get_attrib_tokens())\n",
    "    return -1 if phrase in chemicals and not phrase.isupper() and phrase.lower() not in cd.term_to_sids else 0\n",
    "\n",
    "def LF_bodypart(c):\n",
    "    phrase = re.sub(r's$', '', \" \".join(c[0].get_attrib_tokens()).lower())\n",
    "    return -1 if phrase in bodypart else 0  \n",
    "\n",
    "def LF_protein_chemical_abbrv(c):\n",
    "    '''Gene/protein/chemical name'''\n",
    "    lemma = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return -1 if re.search(\"\\d+\",lemma) else 0\n",
    "\n",
    "def LF_base_pair_seq(c): \n",
    "    lemma = \" \".join(c[0].get_attrib_tokens('lemmas'))\n",
    "    return -1 if re.search(\"^[GACT]{2,}$\",lemma) else 0\n",
    "\n",
    "LFs_false = [LF_chemical_name,\n",
    "             LF_organs,\n",
    "             LF_bodypart,\n",
    "             LF_protein_chemical_abbrv,\n",
    "             LF_base_pair_seq,\n",
    "             #LF_too_vague,\n",
    "             #LF_neg_surfix,\n",
    "             LF_non_common_disease,\n",
    "             LF_non_disease_acronyms,\n",
    "             #LF_pos_in,\n",
    "             LF_gene_chromosome_link,\n",
    "             LF_right_window_incomplete,\n",
    "             #LF_negative_indicator\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    L_train_false_1 = label_manager.load(session, train, 'LF Training Labels -- False 1')\n",
    "except:\n",
    "    L_train_false_1 = label_manager.create(\n",
    "        session, train, 'LF Training Labels -- False 1', LFs_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More neg. LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NEG_COMMON_RGX = r'(finding|disease|syndrome|marker|defecit|.*event|mean|median|mg)s?'\n",
    "def LF_common_neg_phrases(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return -1 if re.match(NEG_COMMON_RGX, p) else 0\n",
    "\n",
    "NEG_AFTER_WORDS = frozenset(['of', 'to'])\n",
    "def LF_neg_after(c):\n",
    "    rw = get_right_tokens(c, window=1, attrib='lemmas')\n",
    "    return -1 if len(rw) > 0 and rw[0] in NEG_AFTER_WORDS and c.disease.get_span().lower() not in cd.term_to_sids else 0\n",
    "\n",
    "def LF_after_num(c):\n",
    "    lw = get_left_tokens(c, window=1, attrib='lemmas')\n",
    "    return -1 if len(lw) > 0 and re.match(r'\\d+', lw[0]) else 0\n",
    "\n",
    "def LF_too_short(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return -1 if len(p) < 3 else 0\n",
    "\n",
    "BAD_ENDINGS_RGX = r'(type|trait|cell)s?$'\n",
    "def LF_bad_endings(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return -1 if re.search(BAD_ENDINGS_RGX, p) else 0\n",
    "\n",
    "BAD_MESH_TERMS = frozenset(['disease', 'diseases', 'conversion'])\n",
    "def LF_bad_MESH_entries(c):\n",
    "    p = c.disease.get_span().lower()\n",
    "    return -1 if p in BAD_MESH_TERMS else 0\n",
    "\n",
    "LFs_false_2 = [\n",
    "    LF_common_neg_phrases,\n",
    "    LF_neg_after,\n",
    "    LF_after_num,\n",
    "    LF_too_short,\n",
    "    LF_bad_endings,\n",
    "    LF_bad_MESH_entries\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    L_train_false_2 = label_manager.load(session, train, 'LF Training Labels -- False 2')\n",
    "except:\n",
    "    L_train_false_2 = label_manager.create(\n",
    "        session, train, 'LF Training Labels -- False 2', LFs_false_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all the LFs\n",
    "\n",
    "Also form the binarized version of the LF matrix for doing DISEASE vs. OTHER tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import binarize_LF_matrix, get_binarized_score\n",
    "from snorkel.annotations import merge_annotations\n",
    "\n",
    "L_train = merge_annotations([\n",
    "        L_train_exact_match\n",
    "        , L_train_drop_leading\n",
    "        , L_train_remove_common\n",
    "        , L_train_cosine_pos\n",
    "        , L_train_cosine_pos_T1\n",
    "        , L_train_cosine_neg\n",
    "        , L_train_false_1\n",
    "        , L_train_false_2\n",
    "    ])\n",
    "\n",
    "L_train_b = binarize_LF_matrix(L_train)\n",
    "L_train_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the generative model\n",
    "\n",
    "For DISEASE vs. OTHER tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model_train = NaiveBayes()\n",
    "%time gen_model_train.train(L_train_b, n_iter=10000, rate=1e-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp_gt_train = gen_model_train.predict(L_train_b, b=0.5)\n",
    "get_binarized_score(yp_gt_train, L_gold_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing LF stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print LF stats...\n",
    "from snorkel.learning import odds_to_prob\n",
    "lfs = L_train.lf_stats(labels=L_gold_train, est_accs=odds_to_prob(gen_model.w))\n",
    "lfs.nlargest(50, \"coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, collect error buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "N_train = L_gold_train.shape[0]\n",
    "\n",
    "fps    = []\n",
    "fns    = []\n",
    "fns_na = []\n",
    "for i in range(N_train):\n",
    "    if yp[i] > 0 and L_gold_train[i] < 0:\n",
    "        fps.append(i)\n",
    "    elif yp[i] <= 0 and L_gold_train[i] > 0:\n",
    "        if yp[i] == 0:\n",
    "            fns_na.append(i)\n",
    "        else:\n",
    "            fns.append(i)\n",
    "\n",
    "shuffle(fps)\n",
    "shuffle(fns)\n",
    "shuffle(fns_na)\n",
    "\n",
    "print len(fps)\n",
    "print len(fns)\n",
    "print len(fns_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, visualize in the `Viewer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "fp_cands = [L_train.get_candidate(i) for i in fps[:100]]\n",
    "svp      = SentenceNgramViewer(fp_cands, session)\n",
    "svp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = svp.get_selected()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the associated labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Label\n",
    "session.query(Label).filter(Label.candidate == c).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.gen_learning import odds_to_prob\n",
    "i = L_train.get_row_index(c)\n",
    "\n",
    "for j in L_train.getrow(i).nonzero()[1]:\n",
    "    print L_train.get_key(j), odds_to_prob(gen_model.w[j]), int(L_train[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Discriminative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import LogReg\n",
    "\n",
    "train_marginals = gen_model_train.marginals(L_train_b)\n",
    "\n",
    "disc_model = LogReg()\n",
    "disc_model.train(F_train, train_marginals, n_iter=2000, rate=1e-3, mu=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp_d_train = disc_model.predict(F_train)\n",
    "get_binarized_score(yp_d_train, L_gold_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp_d_dev = disc_model.predict(F_dev, b=0.5)\n",
    "get_binarized_score(yp_d_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens if we override all _unambiguous_ direct matches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try overriding with any exact matches...\n",
    "yp_d_dev_um = np.zeros(L_gold_dev.shape[0])\n",
    "for i,c in enumerate(dev):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for lf_name, label in LFG_MESH_exact(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    for lf_name, label in LFG_drop_leading_modifiers(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    for lf_name, label in LFG_MESH_exact_remove_common(c):\n",
    "        if label > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    \n",
    "    if neg > 0 and pos == 0:\n",
    "        yp_d_dev_um[i] = -1\n",
    "    elif pos > 0 and neg == 0:\n",
    "        yp_d_dev_um[i] = 1\n",
    "    else:\n",
    "        yp_d_dev_um[i] = yp_d_dev[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_binarized_score(yp_d_dev_um, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can also train the multinomial generative model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.learning_mn import assemble_mn_format, LogReg\n",
    "\n",
    "# Get data in the correct format\n",
    "Xs, mn_maps, mn_inv_maps, nz_idxs = assemble_mn_format(L_train)\n",
    "\n",
    "# Run multinomial model\n",
    "gen_model = LogReg()\n",
    "gen_model.train(Xs, n_iter=100, rate=1e-2, w0=np.ones(L_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import get_mn_score\n",
    "\n",
    "train_marginals = gen_model.marginals(Xs)\n",
    "N_pos_train     = sum([1 for i in range(L_gold_train.shape[0]) if L_gold_train[i,0] > 0])\n",
    "predicted       = [mn_inv_maps[i][np.argmax(m)] for i,m in enumerate(train_marginals)]\n",
    "get_mn_score(predicted, L_gold_train[nz_idxs], N_total_pos=N_pos_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
