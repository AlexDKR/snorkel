{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial, Part I: Candidate Extraction\n",
    "\n",
    "In this example, we'll be writing an application to extract **person-age relationships** from homemade tables, as per the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  At core, we will be constructing a model to classify _person-age relation mentions_ as either true or false.  To do this, we first need a set of such candidates- in this notebook, we'll use `Snorkel` utilities to extract person candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Corpus\n",
    "\n",
    "First, we will load and pre-process the corpus, storing it for convenience in a `Corpus` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a table parser\n",
    "\n",
    "We'll start by defining an 'HTMLTableParser' class to read HTML tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ddlite_parser import HTMLTableParser\n",
    "html_parser = HTMLTableParser(\n",
    "    path='data/diseases_table.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a cell parser\n",
    "\n",
    "Next, we'll use an NLP preprocessing tool to split the `Document` objects into sentences, tokens, and provide annotations--part-of-speech tags, dependency parse structure, lemmatized word forms, etc.--for these sentences.  Here we use the default `SentenceParser` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_parser import TableParser\n",
    "table_parser = TableParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing & loading the corpus\n",
    "\n",
    "Finally, we'll put this all together using a `Corpus` object, which will execute the parsers and store the results as an iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing documents...\n",
      "Parsing sentences...\n",
      "CPU times: user 55.1 ms, sys: 5.08 ms, total: 60.2 ms\n",
      "Wall time: 122 ms\n"
     ]
    }
   ],
   "source": [
    "from ddlite_parser import Corpus\n",
    "%time corpus = Corpus(html_parser, table_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=0, file='diseases_table.xml', text='<table>  <tr>    <th>Disease</th>    <th>Location</th>    <th>Year</th>  </tr>  <tr>    <th>Polio</th>    <td>New York</td>    <td>1914</td>  </tr>  <tr>    <th>Chicken Pox</th>    <td>Boston</td>    <td>2001</td>  </tr>  <tr>    <th>Scurvy</th>    <td>Annapolis</td>    <td>1901</td>  </tr>  <caption>    Table 1: Infectious diseases and where to find them.  </caption></table>', attribs=None),\n",
       " Document(id=1, file='diseases_table.xml', text='<table>  <tr>    <th>Problem</th>    <th>Cause</th>    <th>Cost</th>  </tr>  <tr>    <th>Arthritis</th>    <td>Pokemon Go</td>    <td>Free</td>  </tr>  <tr>    <th>Yellow Fever</th>    <td>Unicorns</td>    <td>$17.75</td>  </tr>  <tr>    <th>Hypochondria</th>    <td>Fear</td>    <td>$100</td>  </tr>  <caption>    Table 2: Three ways to get sick and pay for it.  </caption></table>', attribs=None)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(id=0, file='diseases_table.xml', text='<table>  <tr>    <th>Disease</th>    <th>Location</th>    <th>Year</th>  </tr>  <tr>    <th>Polio</th>    <td>New York</td>    <td>1914</td>  </tr>  <tr>    <th>Chicken Pox</th>    <td>Boston</td>    <td>2001</td>  </tr>  <tr>    <th>Scurvy</th>    <td>Annapolis</td>    <td>1901</td>  </tr>  <caption>    Table 1: Infectious diseases and where to find them.  </caption></table>', attribs=None)\n"
     ]
    }
   ],
   "source": [
    "doc  = corpus.get_docs()[0]\n",
    "print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell(id='0-7', doc_id=0, sent_id=0, doc_name='diseases_table.xml', words=[u'Boston'], lemmas=[u'Boston'], poses=[u'NNP'], dep_parents=[0], dep_labels=[u'ROOT'], char_offsets=[0], text=u'Boston', row_num=2, col_num=1, header_tag=False, meta_cell=False)\n"
     ]
    }
   ],
   "source": [
    "sent = corpus.get_sentences_in(doc.id)[7]\n",
    "print sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a basic candidate extractor\n",
    "\n",
    "Next, we'll write a basic function to extract **candidate disease mentions** from the corpus.  For this first attempt, we'll just write a function that checks for matches against a list (or _\"dictionary\"_) of disease phrases, constructed using some pre-compiled ontologies ([UMLS](https://www.nlm.nih.gov/research/umls/), [ORDO](http://www.orphadata.org/cgi-bin/inc/ordo_orphanet.inc.php), [DOID](http://www.obofoundry.org/ontology/doid.html), [NCBI Diseases](http://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/); see `tutorial/data/diseases.py`).\n",
    "\n",
    "We'll do this using a `CandidateSpace` object--which defines the basic candidates we consider, in this case n-grams up to a certain length--and a `Matcher` object, which filters this candidate space down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 507899 disease phrases!\n"
     ]
    }
   ],
   "source": [
    "from load_dictionaries import load_disease_dictionary\n",
    "from ddlite_candidates import Ngrams\n",
    "from ddlite_matchers import DictionaryMatch\n",
    "\n",
    "# Load the disease phrase dictionary\n",
    "diseases = load_disease_dictionary()\n",
    "print \"Loaded %s disease phrases!\" % len(diseases)\n",
    "\n",
    "# Define a candidate space\n",
    "ngrams = Ngrams(n_max=3)\n",
    "\n",
    "# Define a matcher\n",
    "matcher = DictionaryMatch(d=diseases, longest_match_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that we set `longest_match_only=False`, which means that we _will_ consider subsequences of phrases that match \n",
    "our dictionary.\n",
    "\n",
    "The `Ngrams` operator is applied over our `Sentence` objects and returns `Ngram` objects, and the `Matcher` then filters these, so we apply our operators over the sentences in the corpus, storing the results in a `Candidates` object for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Cell(id='1-9', doc_id=1, sent_id=0, doc_name='diseases_table.xml', words=[u'Hypochondria'], lemmas=[u'Hypochondria'], poses=[u'NNP'], dep_parents=[0], dep_labels=[u'ROOT'], char_offsets=[0], text=u'Hypochondria', row_num=3, col_num=0, header_tag=True, meta_cell=False),\n",
       " Cell(id='1-8', doc_id=1, sent_id=0, doc_name='diseases_table.xml', words=[u'$', u'17.75'], lemmas=[u'$', u'17.75'], poses=[u'$', u'CD'], dep_parents=[0, 1], dep_labels=[u'ROOT', u'nummod'], char_offsets=[0, 1], text=u'$17.75', row_num=2, col_num=2, header_tag=False, meta_cell=False),\n",
       " Cell(id='1-11', doc_id=1, sent_id=0, doc_name='diseases_table.xml', words=[u'$', u'100'], lemmas=[u'$', u'100'], poses=[u'$', u'CD'], dep_parents=[0, 1], dep_labels=[u'ROOT', u'nummod'], char_offsets=[0, 1], text=u'$100', row_num=3, col_num=2, header_tag=False, meta_cell=False),\n",
       " Cell(id='1-10', doc_id=1, sent_id=0, doc_name='diseases_table.xml', words=[u'Fear'], lemmas=[u'fear'], poses=[u'NN'], dep_parents=[0], dep_labels=[u'ROOT'], char_offsets=[0], text=u'Fear', row_num=3, col_num=1, header_tag=False, meta_cell=False),\n",
       " Cell(id='1-12', doc_id=1, sent_id=0, doc_name='diseases_table.xml', words=[u'Table', u'2', u':', u'Three', u'ways', u'to', u'get', u'sick', u'and', u'pay', u'for', u'it', u'.'], lemmas=[u'Table', u'2', u':', u'three', u'way', u'to', u'get', u'sick', u'and', u'pay', u'for', u'it', u'.'], poses=[u'NNP', u'CD', u':', u'CD', u'NNS', u'TO', u'VB', u'JJ', u'CC', u'NN', u'IN', u'PRP', u'.'], dep_parents=[0, 1, 1, 5, 1, 7, 5, 7, 8, 8, 12, 7, 1], dep_labels=[u'ROOT', u'nummod', u'punct', u'nummod', u'dep', u'mark', u'acl', u'xcomp', u'cc', u'conj', u'case', u'nmod', u'punct'], char_offsets=[4, 10, 11, 13, 19, 24, 27, 31, 36, 40, 44, 48, 50], text=u'Table 2: Three ways to get sick and pay for it.', row_num=-1, col_num=-1, header_tag=False, meta_cell=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_sentences()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting candidates...\n",
      "CPU times: user 5.03 ms, sys: 2.11 ms, total: 7.14 ms\n",
      "Wall time: 6.03 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Ngram(\"Hypochondria\", id=1-9:0-11, chars=[0,11], words=[0,0]),\n",
       " <Ngram(\"Yellow Fever\", id=1-6:0-11, chars=[0,11], words=[0,1]),\n",
       " <Ngram(\"Polio\", id=0-3:0-4, chars=[0,4], words=[0,0]),\n",
       " <Ngram(\"Arthritis\", id=1-3:0-8, chars=[0,8], words=[0,0]),\n",
       " <Ngram(\"diseases\", id=0-12:24-31, chars=[24,31], words=[4,4]),\n",
       " <Ngram(\"Infectious diseases\", id=0-12:13-31, chars=[13,31], words=[3,4]),\n",
       " <Ngram(\"Fever\", id=1-6:7-11, chars=[7,11], words=[1,1]),\n",
       " <Ngram(\"Scurvy\", id=0-9:0-5, chars=[0,5], words=[0,0]),\n",
       " <Ngram(\"Chicken Pox\", id=0-6:0-10, chars=[0,10], words=[0,1]),\n",
       " <Ngram(\"Problem\", id=1-0:0-6, chars=[0,6], words=[0,0]),\n",
       " <Ngram(\"Disease\", id=0-0:0-6, chars=[0,6], words=[0,0])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ddlite_candidates import Candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.get_candidates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our candidate recall on gold annotations\n",
    "\n",
    "Next, we'll test our _candidate recall_--in other words, how many of the true disease mentions we picked up in our candidate set--using the gold annotations in our dataset.\n",
    "\n",
    "The XML documents that we loaded using the `XMLDocParser` also contained annotations (this is why we kept the full xml tree using `keep_xml_tree=True`).  We'll load these annotations and map them to `Ngram` objects over our parsed sentences, that way we can easily compare our extracted candidate set with the gold annotations.  The code is fairly simple (see `tutorial/util.py`); note that we filter to only keep _disease_ annotations, and that the candidates should be uniquely identified by their `id` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-86ff9aa69fd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgold\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollect_pubtator_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Disease'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bradenhancock/snorkel/tutorial/utils.py\u001b[0m in \u001b[0;36mcollect_pubtator_annotations\u001b[0;34m(doc, sents, sep)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Get Ngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'root'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.//annotation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Relation annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "from utils import collect_pubtator_annotations\n",
    "gold = []\n",
    "for doc, sents in corpus:\n",
    "    gold += [a for a in collect_pubtator_annotations(doc, sents) if a.metadata['type'] == 'Disease']\n",
    "gold = frozenset(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a set of gold annotations of the same type as our candidates (`Ngram`), and can use set operations (where candidate objects are hashed by their `id` attribute), e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(gold.intersection(c.get_candidates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we'll use a basic helper method of the `Candidates` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that our focus in this stage is on **acheiving high candidate recall, without considering an impractically large candidate set**.  Our main focus after this stage will be on training a classifier to select which candidates are true; this will raise precision while hopefully keeping recall high.  _Note however that candidate recall is an upper bound for the recall of this classifier!_\n",
    "\n",
    "So, we have some work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `Viewer` to inspect data\n",
    "\n",
    "Next, we'll use the `Viewer` class--here, specifically, the `SentenceNgramViewer`--to inspect the data.\n",
    "\n",
    "To start, we'll assemble a random set of all the sentences where there are gold annotations _not in our candidate set_, i.e. where we missed something, and then inspect these in the `Viewer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-0b193dd15234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Index the gold annotations by sentence id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgold_by_sid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mgold_by_sid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gold' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "\n",
    "# Index the gold annotations by sentence id\n",
    "gold_by_sid = defaultdict(list)\n",
    "for g in gold:\n",
    "    gold_by_sid[g.sent_id].append(g)\n",
    "\n",
    "# Get sentences\n",
    "view_sents = [s for s in corpus.get_sentences() if len(c.get_candidates_in(s.id)) < len(gold_by_sid[s.id])]\n",
    "shuffle(view_sents)\n",
    "view_sents = view_sents[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate and render the `Viewer` object; note we're being a bit sloppy, passing in _all_ the candidates and gold labels, but the `Viewer` object will take care of indexing them by sentence, and will only render the sentences we pass in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ddlite_viewer import SentenceNgramViewer\n",
    "sv = SentenceNgramViewer(view_sents, c.get_candidates(), gold=gold)\n",
    "sv.render(n_per_page=3, height=225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing a better candidate extractor\n",
    "\n",
    "Let's try to increase our candidate recall using more of the `Matcher` operators and their functionalities.  First, let's turn on **Porter stemming** in our dictionary matcher; Porter stemming is an aggressive rules-based method for normalizing word endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a new matcher\n",
    "matcher = DictionaryMatch(d=diseases, longest_match_only=False, stemmer='porter')\n",
    "\n",
    "# Extract a new set of candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, note that *`Matcher` objects are compositional*. Observing in the `Viewer` that we are missing all of the acronyms, let's start with the `Union` operator, to integrate a dictionary for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_matchers import Union\n",
    "from load_dictionaries import load_acronym_dictionary\n",
    "\n",
    "# Load the disease phrase dictionary\n",
    "acronyms = load_acronym_dictionary()\n",
    "print \"Loaded %s acronyms!\" % len(acronyms)\n",
    "\n",
    "# Define a new matcher\n",
    "matcher = Union(\n",
    "    DictionaryMatch(d=diseases, longest_match_only=False, stemmer='porter'),\n",
    "    DictionaryMatch(d=acronyms, ignore_case=False))\n",
    "\n",
    "# Extract a new set of candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try using the `Concat` and `RegexMatch` operators to find candidate mentions composed of an _adjective followed by a term matching our diseases dictionary_.  Note in particular that we set `left_required=False` so that exact matches to our dictionary (with no adjective prepended) will still work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_matchers import Concat, RegexMatchEach\n",
    "matcher = Union(\n",
    "    Concat(\n",
    "        RegexMatchEach(rgx=r'JJ*', attrib='poses'),\n",
    "        DictionaryMatch(d=diseases, longest_match_only=False, stemmer='porter'),\n",
    "        left_required=False),\n",
    "    DictionaryMatch(d=acronyms, ignore_case=False))\n",
    "\n",
    "# Extract a new set of candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running candidate extraction in parallel\n",
    "\n",
    "Note that **the candidate extraction procedure can be parallelized across multiple cores using the `parallelism=N`** optional argument to the `Candidates` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More coming here...\n",
    "\n",
    "We've increased the candidate recall (on the development set) by 7.3% using some simple compositional `Matcher` operators.  We'll be adding more here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting with the rest of the `DDLite` workflow\n",
    "\n",
    "We are in the process of a big code refactor!  For now, to connect the candidates derived as in above to the rest of the `DDLite` workflow, create a `CandidateExtractor` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ddlite_matchers import CandidateExtractor\n",
    "ce = CandidateExtractor(ngrams, matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object can now be used in place of the candidate extractors in the other tutorials!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
