{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial, Part I: Candidate Extraction\n",
    "\n",
    "In this example, we'll be writing an application to extract **person-age relationships** from homemade tables, as per the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  At core, we will be constructing a model to classify _person-age relation mentions_ as either true or false.  To do this, we first need a set of such candidates- in this notebook, we'll use `Snorkel` utilities to extract person candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Corpus\n",
    "\n",
    "First, we will load and pre-process the corpus, storing it for convenience in a `Corpus` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a table parser\n",
    "\n",
    "We'll start by defining an 'HTMLTableParser' class to read HTML tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ddlite_parser import HTMLTableParser\n",
    "html_parser = HTMLTableParser(\n",
    "    path='data/Tables_DevelopmentSet.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a cell parser\n",
    "\n",
    "Next, we'll use an NLP preprocessing tool to split the `Document` objects into sentences, tokens, and provide annotations--part-of-speech tags, dependency parse structure, lemmatized word forms, etc.--for these sentences.  Here we use the default `SentenceParser` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_parser import SentenceParser\n",
    "sent_parser = SentenceParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing & loading the corpus\n",
    "\n",
    "Finally, we'll put this all together using a `Corpus` object, which will execute the parsers and store the results as an iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing documents...\n",
      "> /Users/bradenhancock/snorkel/ddlite_parser.py(235)__init__()\n",
      "-> print \"Parsing sentences...\"\n",
      "(Pdb) self._docs_by_id\n",
      "{<built-in function id>: Document(id=<built-in function id>, file='Tables_DevelopmentSet.xml', text=<table>\\n<tr>\\n<th>Person</th>\\n<th>Age</th>\\n<th>Birthdate</th>\\n</tr>\\n<tr>\\n<th>Alice</th>\\n<td>45</td>\\n<td>Mar. 15, 1971</td>\\n</tr>\\n<tr>\\n<th>Bob</th>\\n<td>17</td>\\n<td>Jan. 1, 1999</td>\\n</tr>\\n<tr>\\n<th>Charlie</th>\\n<td>80</td>\\n<td>July 13, 1936</td>\\n</tr>\\n<caption>\\n    Table 1: Ages and birthdates of the world's most important people.\\n  </caption>\\n</table>, attribs=None)}\n",
      "(Pdb) self._docs_by_id[0]\n",
      "*** KeyError: 0\n",
      "(Pdb) self._docs_by_id.values()\n",
      "[Document(id=<built-in function id>, file='Tables_DevelopmentSet.xml', text=<table>\\n<tr>\\n<th>Person</th>\\n<th>Age</th>\\n<th>Birthdate</th>\\n</tr>\\n<tr>\\n<th>Alice</th>\\n<td>45</td>\\n<td>Mar. 15, 1971</td>\\n</tr>\\n<tr>\\n<th>Bob</th>\\n<td>17</td>\\n<td>Jan. 1, 1999</td>\\n</tr>\\n<tr>\\n<th>Charlie</th>\\n<td>80</td>\\n<td>July 13, 1936</td>\\n</tr>\\n<caption>\\n    Table 1: Ages and birthdates of the world's most important people.\\n  </caption>\\n</table>, attribs=None)]\n",
      "(Pdb) self._docs_by_id.values()[0]\n",
      "Document(id=<built-in function id>, file='Tables_DevelopmentSet.xml', text=<table>\\n<tr>\\n<th>Person</th>\\n<th>Age</th>\\n<th>Birthdate</th>\\n</tr>\\n<tr>\\n<th>Alice</th>\\n<td>45</td>\\n<td>Mar. 15, 1971</td>\\n</tr>\\n<tr>\\n<th>Bob</th>\\n<td>17</td>\\n<td>Jan. 1, 1999</td>\\n</tr>\\n<tr>\\n<th>Charlie</th>\\n<td>80</td>\\n<td>July 13, 1936</td>\\n</tr>\\n<caption>\\n    Table 1: Ages and birthdates of the world's most important people.\\n  </caption>\\n</table>, attribs=None)\n"
     ]
    }
   ],
   "source": [
    "from ddlite_parser import Corpus\n",
    "%time corpus = Corpus(html_parser, sent_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc  = corpus.get_docs()[0]\n",
    "print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent = corpus.get_sentences_in(doc.id)[0]\n",
    "print sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a basic candidate extractor\n",
    "\n",
    "Next, we'll write a basic function to extract **candidate disease mentions** from the corpus.  For this first attempt, we'll just write a function that checks for matches against a list (or _\"dictionary\"_) of disease phrases, constructed using some pre-compiled ontologies ([UMLS](https://www.nlm.nih.gov/research/umls/), [ORDO](http://www.orphadata.org/cgi-bin/inc/ordo_orphanet.inc.php), [DOID](http://www.obofoundry.org/ontology/doid.html), [NCBI Diseases](http://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/); see `tutorial/data/diseases.py`).\n",
    "\n",
    "We'll do this using a `CandidateSpace` object--which defines the basic candidates we consider, in this case n-grams up to a certain length--and a `Matcher` object, which filters this candidate space down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from load_dictionaries import load_disease_dictionary\n",
    "from ddlite_candidates import Ngrams\n",
    "from ddlite_matchers import DictionaryMatch\n",
    "\n",
    "# Load the disease phrase dictionary\n",
    "diseases = load_disease_dictionary()\n",
    "print \"Loaded %s disease phrases!\" % len(diseases)\n",
    "\n",
    "# Define a candidate space\n",
    "ngrams = Ngrams(n_max=3)\n",
    "\n",
    "# Define a matcher\n",
    "matcher = DictionaryMatch(d=diseases, longest_match_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we set `longest_match_only=False`, which means that we _will_ consider subsequences of phrases that match our dictionary.\n",
    "\n",
    "The `Ngrams` operator is applied over our `Sentence` objects and returns `Ngram` objects, and the `Matcher` then filters these, so we apply our operators over the sentences in the corpus, storing the results in a `Candidates` object for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_candidates import Candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.get_candidates()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our candidate recall on gold annotations\n",
    "\n",
    "Next, we'll test our _candidate recall_--in other words, how many of the true disease mentions we picked up in our candidate set--using the gold annotations in our dataset.\n",
    "\n",
    "The XML documents that we loaded using the `XMLDocParser` also contained annotations (this is why we kept the full xml tree using `keep_xml_tree=True`).  We'll load these annotations and map them to `Ngram` objects over our parsed sentences, that way we can easily compare our extracted candidate set with the gold annotations.  The code is fairly simple (see `tutorial/util.py`); note that we filter to only keep _disease_ annotations, and that the candidates should be uniquely identified by their `id` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import collect_pubtator_annotations\n",
    "gold = []\n",
    "for doc, sents in corpus:\n",
    "    gold += [a for a in collect_pubtator_annotations(doc, sents) if a.metadata['type'] == 'Disease']\n",
    "gold = frozenset(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a set of gold annotations of the same type as our candidates (`Ngram`), and can use set operations (where candidate objects are hashed by their `id` attribute), e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(gold.intersection(c.get_candidates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we'll use a basic helper method of the `Candidates` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that our focus in this stage is on **acheiving high candidate recall, without considering an impractically large candidate set**.  Our main focus after this stage will be on training a classifier to select which candidates are true; this will raise precision while hopefully keeping recall high.  _Note however that candidate recall is an upper bound for the recall of this classifier!_\n",
    "\n",
    "So, we have some work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `Viewer` to inspect data\n",
    "\n",
    "Next, we'll use the `Viewer` class--here, specifically, the `SentenceNgramViewer`--to inspect the data.\n",
    "\n",
    "To start, we'll assemble a random set of all the sentences where there are gold annotations _not in our candidate set_, i.e. where we missed something, and then inspect these in the `Viewer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "\n",
    "# Index the gold annotations by sentence id\n",
    "gold_by_sid = defaultdict(list)\n",
    "for g in gold:\n",
    "    gold_by_sid[g.sent_id].append(g)\n",
    "\n",
    "# Get sentences\n",
    "view_sents = [s for s in corpus.get_sentences() if len(c.get_candidates_in(s.id)) < len(gold_by_sid[s.id])]\n",
    "shuffle(view_sents)\n",
    "view_sents = view_sents[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate and render the `Viewer` object; note we're being a bit sloppy, passing in _all_ the candidates and gold labels, but the `Viewer` object will take care of indexing them by sentence, and will only render the sentences we pass in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ddlite_viewer import SentenceNgramViewer\n",
    "sv = SentenceNgramViewer(view_sents, c.get_candidates(), gold=gold)\n",
    "sv.render(n_per_page=3, height=225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing a better candidate extractor\n",
    "\n",
    "Let's try to increase our candidate recall using more of the `Matcher` operators and their functionalities.  First, let's turn on **Porter stemming** in our dictionary matcher; Porter stemming is an aggressive rules-based method for normalizing word endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a new matcher\n",
    "matcher = DictionaryMatch(d=diseases, longest_match_only=False, stemmer='porter')\n",
    "\n",
    "# Extract a new set of candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, note that *`Matcher` objects are compositional*. Observing in the `Viewer` that we are missing all of the acronyms, let's start with the `Union` operator, to integrate a dictionary for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_matchers import Union\n",
    "from load_dictionaries import load_acronym_dictionary\n",
    "\n",
    "# Load the disease phrase dictionary\n",
    "acronyms = load_acronym_dictionary()\n",
    "print \"Loaded %s acronyms!\" % len(acronyms)\n",
    "\n",
    "# Define a new matcher\n",
    "matcher = Union(\n",
    "    DictionaryMatch(d=diseases, longest_match_only=False, stemmer='porter'),\n",
    "    DictionaryMatch(d=acronyms, ignore_case=False))\n",
    "\n",
    "# Extract a new set of candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try using the `Concat` and `RegexMatch` operators to find candidate mentions composed of an _adjective followed by a term matching our diseases dictionary_.  Note in particular that we set `left_required=False` so that exact matches to our dictionary (with no adjective prepended) will still work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_matchers import Concat, RegexMatchEach\n",
    "matcher = Union(\n",
    "    Concat(\n",
    "        RegexMatchEach(rgx=r'JJ*', attrib='poses'),\n",
    "        DictionaryMatch(d=diseases, longest_match_only=False, stemmer='porter'),\n",
    "        left_required=False),\n",
    "    DictionaryMatch(d=acronyms, ignore_case=False))\n",
    "\n",
    "# Extract a new set of candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running candidate extraction in parallel\n",
    "\n",
    "Note that **the candidate extraction procedure can be parallelized across multiple cores using the `parallelism=N`** optional argument to the `Candidates` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More coming here...\n",
    "\n",
    "We've increased the candidate recall (on the development set) by 7.3% using some simple compositional `Matcher` operators.  We'll be adding more here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting with the rest of the `DDLite` workflow\n",
    "\n",
    "We are in the process of a big code refactor!  For now, to connect the candidates derived as in above to the rest of the `DDLite` workflow, create a `CandidateExtractor` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ddlite_matchers import CandidateExtractor\n",
    "ce = CandidateExtractor(ngrams, matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object can now be used in place of the candidate extractors in the other tutorials!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
