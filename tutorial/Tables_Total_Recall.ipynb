{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables Total Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to get to the point where we can extract 100% of part names from transistor hardware sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.parser import HTMLParser\n",
    "from snorkel.parser import TableParser, OmniParser\n",
    "from snorkel.candidates import Ngrams, NgramsWithRanges, EntityExtractor\n",
    "from snorkel.matchers import RegexMatchEach, DictionaryMatch, RangeMatcher\n",
    "from utils import collect_hardware_doc_part_pairs\n",
    "\n",
    "def parse_corpus(parser, max_docs=101):\n",
    "    doc_parser = HTMLParser(path='data/hardware/hardware_html/')\n",
    "    context_parser = OmniParser()\n",
    "    cp = CorpusParser(doc_parser, context_parser, max_docs=max_docs)\n",
    "    corpus = cp.parse_corpus(name='Hardware Corpus')\n",
    "    print \"Corpus has been parsed.\"\n",
    "    return corpus\n",
    "\n",
    "def load_corpus(parser):\n",
    "    if isinstance(parser, TableParser):\n",
    "        filename = \"data/hardware/hardware_corpus_table.pkl\"\n",
    "    elif isinstance(parser, OmniParser):\n",
    "        filename = \"data/hardware/hardware_corpus_omni.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"Input must be of type TableParser or OmniParser.\")\n",
    "    try:\n",
    "        with open(filename,\"r\") as pkl:\n",
    "            return cPickle.load(pkl)\n",
    "        print \"Corpus has been loaded.\"\n",
    "    except:\n",
    "        print \"Corpus could not be loaded.\"\n",
    "        return None\n",
    "\n",
    "def save_corpus(corpus, parser):\n",
    "    if isinstance(parser, TableParser):\n",
    "        filename = \"data/hardware/hardware_corpus_table.pkl\"\n",
    "    elif isinstance(parser, OmniParser):\n",
    "        filename = \"data/hardware/hardware_corpus_omni.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"Input must be of type TableParser or OmniParser.\")\n",
    "    with open(filename,\"w\") as pkl:\n",
    "        %time cPickle.dump(corpus, pkl)\n",
    "        print \"Corpus has been pickled.\"\n",
    "\n",
    "def load_gold():\n",
    "    filename='data/hardware/gold_all.csv'\n",
    "    gold_pairs = collect_hardware_doc_part_pairs(filename)\n",
    "    return gold_pairs\n",
    "        \n",
    "def extract_part_numbers(corpus, cand_space, gold_parts):\n",
    "    part_matcher = DictionaryMatch(d=gold_parts)\n",
    "    part_extractor = EntityExtractor(cand_space, part_matcher)\n",
    "    parts = part_extractor.extract(corpus.get_phrases(), name='all')\n",
    "    return parts\n",
    "\n",
    "def print_stats(g, x):\n",
    "    tp = len(g.intersection(x))\n",
    "    fp = len(x.difference(g))\n",
    "    fn = len(g.difference(x))\n",
    "    precision = float(tp)/(tp + fp)\n",
    "    recall = float(tp)/(tp + fn)\n",
    "    print \"Precision: %0.3f (%s/%s)\" % (precision, tp, tp + fp)\n",
    "    print \"Recall: %0.3f (%s/%s)\" % (recall, tp, tp + fn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 542 gold (doc, part) pairs.\n"
     ]
    }
   ],
   "source": [
    "gold_loaded = True\n",
    "\n",
    "if gold_loaded:\n",
    "    try:\n",
    "        print \"Using %s gold (doc, part) pairs.\" % len(gold_pairs)\n",
    "    except:\n",
    "        print \"Gold data not in memory yet.\"\n",
    "        gold_loaded = False\n",
    "if not gold_loaded:\n",
    "    gold_pairs = load_gold()\n",
    "    (gold_docs, gold_parts) = zip(*gold_pairs)\n",
    "    # make gold_parts_suffixed for matcher\n",
    "    gold_parts_extended = []\n",
    "    for part in gold_parts:\n",
    "        for suffix in ['', 'A','B','C','-16','-25','-40']:\n",
    "            gold_parts_extended.append(''.join([part,suffix]))\n",
    "            if part.endswith(suffix):\n",
    "                gold_parts_extended.append(part[:-len(suffix)])\n",
    "                if part[:2].isalpha() and part[2:-1].isdigit() and part[-1].isalpha():\n",
    "                    gold_parts_extended.append(' '.join([part[:2], part[2:-1], part[-1]]))\n",
    "    print \"Loaded %s gold (doc, part) pairs.\" % len(gold_pairs)\n",
    "    print \"Dictionary containts %d potential part numbers.\" % len(gold_parts_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus in memory was parsed with OmniParser\n",
      "Loading corpus for TableParser...\n",
      "CPU times: user 15.5 s, sys: 2.21 s, total: 17.7 s\n",
      "Wall time: 18.3 s\n",
      "Corpus loaded with 101 documents\n"
     ]
    }
   ],
   "source": [
    "corpus_loaded = True\n",
    "\n",
    "if corpus_loaded:\n",
    "    try:\n",
    "        if isinstance(parser, TableParser):\n",
    "            print \"Corpus from %s in memory with %d documents\" \\\n",
    "            % (type(parser).__name__, len(corpus.documents))\n",
    "        else:\n",
    "            print \"Corpus in memory was parsed with %s\"% type(parser).__name__\n",
    "            corpus_loaded = False\n",
    "    except:\n",
    "        print \"Corpus not in memory yet.\"\n",
    "        corpus_loaded = False\n",
    "if not corpus_loaded:\n",
    "    parser = TableParser()\n",
    "    print \"Loading corpus for %s...\" % type(parser).__name__\n",
    "    %time corpus = load_corpus(parser)\n",
    "    print \"Corpus loaded with %d documents\" % len(corpus.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.62 s, sys: 48.9 ms, total: 1.67 s\n",
      "Wall time: 1.66 s\n",
      "Extracted 1312 candidate part numbers.\n",
      "\n",
      "Part Stats:\n",
      "Precision: 0.852 (144/169)\n",
      "Recall: 0.804 (144/179)\n",
      "\n",
      "Pair Stats:\n",
      "Precision: 0.592 (251/424)\n",
      "Recall: 0.463 (251/542)\n"
     ]
    }
   ],
   "source": [
    "cand_space = Ngrams(n_max=3)\n",
    "%time parts = extract_part_numbers(corpus, cand_space, gold_parts_extended)\n",
    "print \"Extracted %s candidate part numbers.\" % len(parts)\n",
    "\n",
    "g_table_parts = set(gold_parts)\n",
    "x_table_parts = set([p.get_span() for p in parts])\n",
    "print \"\\nPart Stats:\"\n",
    "print_stats(g_table_parts, x_table_parts)\n",
    "\n",
    "g_table_pairs = set(gold_pairs)\n",
    "x_table_pairs = set([(p.context.document.name, p.get_span()) for p in parts])\n",
    "print \"\\nPair Stats:\"\n",
    "print_stats(g_table_pairs, x_table_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OmniParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bradenhancock/snorkel/snorkel/parser.py:204: RuntimeWarning: Submission from file PHGLS20125-1 too long. Max character count is 100K. Submission was skipped.\n",
      "  warnings.warn(\"Submission from file {} too long. Max character count is 100K. Submission was skipped.\".format(doc.name), RuntimeWarning)\n",
      "/Users/bradenhancock/snorkel/snorkel/parser.py:204: RuntimeWarning: Submission from file PHGLS20126-1 too long. Max character count is 100K. Submission was skipped.\n",
      "  warnings.warn(\"Submission from file {} too long. Max character count is 100K. Submission was skipped.\".format(doc.name), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has been parsed.\n",
      "CPU times: user 3min 45s, sys: 13.1 s, total: 3min 58s\n",
      "Wall time: 6min 29s\n",
      "CPU times: user 25.1 s, sys: 2.4 s, total: 27.5 s\n",
      "Wall time: 28.9 s\n",
      "Corpus has been pickled.\n"
     ]
    }
   ],
   "source": [
    "reparse_corpus = True\n",
    "save_new_corpus = True\n",
    "\n",
    "if reparse_corpus:\n",
    "    parser = OmniParser()\n",
    "    %time corpus = parse_corpus(parser, max_docs=101)\n",
    "    if save_new_corpus:\n",
    "        save_corpus(corpus, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus in memory was parsed with TableParser\n",
      "Loading corpus for OmniParser...\n",
      "CPU times: user 29 s, sys: 2.62 s, total: 31.7 s\n",
      "Wall time: 33.7 s\n",
      "Corpus loaded with 101 documents\n"
     ]
    }
   ],
   "source": [
    "corpus_loaded = True\n",
    "\n",
    "if corpus_loaded:\n",
    "    try:\n",
    "        if isinstance(parser, OmniParser):\n",
    "            print \"Corpus from %s in memory with %d documents\" \\\n",
    "            % (type(parser).__name__, len(corpus.documents))\n",
    "        else:\n",
    "            print \"Corpus in memory was parsed with %s\"% type(parser).__name__\n",
    "            corpus_loaded = False\n",
    "    except:\n",
    "        print \"Corpus not in memory yet.\"\n",
    "        corpus_loaded = False\n",
    "if not corpus_loaded:\n",
    "    parser = OmniParser()\n",
    "    print \"Loading corpus for %s...\" % type(parser).__name__\n",
    "    %time corpus = load_corpus(parser)\n",
    "    print \"Corpus loaded with %d documents\" % len(corpus.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.97 s, sys: 80 ms, total: 9.05 s\n",
      "Wall time: 9.13 s\n",
      "Extracted 1912 candidate part numbers.\n",
      "\n",
      "Part Stats:\n",
      "Precision: 1.000 (167/167)\n",
      "Recall: 0.933 (167/179)\n",
      "\n",
      "Pair Stats:\n",
      "Precision: 0.738 (385/522)\n",
      "Recall: 0.710 (385/542)\n"
     ]
    }
   ],
   "source": [
    "cand_space = Ngrams(n_max=3)\n",
    "%time parts = extract_part_numbers(corpus, cand_space, gold_parts)\n",
    "print \"Extracted %s candidate part numbers.\" % len(parts)\n",
    "\n",
    "g_omni_parts = set(gold_parts)\n",
    "x_omni_parts = set([p.get_span().upper() for p in parts])\n",
    "print \"\\nPart Stats:\"\n",
    "print_stats(g_omni_parts, x_omni_parts)\n",
    "\n",
    "g_omni_pairs = set(gold_pairs)\n",
    "x_omni_pairs = set([(p.context.document.name, p.get_span().upper()) for p in parts])\n",
    "print \"\\nPair Stats:\"\n",
    "print_stats(g_omni_pairs, x_omni_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BC547A', 'BC547C', 'BC547B']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.utils import expand_implicit_text\n",
    "list(expand_implicit_text(\"BC547A/B/C\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.9 s, sys: 1.11 s, total: 44 s\n",
      "Wall time: 55.9 s\n",
      "Extracted 6458 candidate part numbers.\n",
      "\n",
      "Part Stats:\n",
      "Precision: 0.869 (172/198)\n",
      "Recall: 0.961 (172/179)\n",
      "\n",
      "Pair Stats:\n",
      "Precision: 0.627 (416/664)\n",
      "Recall: 0.768 (416/542)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.candidates import NgramsWithRanges\n",
    "cand_space = NgramsWithRanges(n_max=3)\n",
    "%time parts = extract_part_numbers(corpus, cand_space, gold_parts_extended)\n",
    "print \"Extracted %s candidate part numbers.\" % len(parts)\n",
    "\n",
    "g_implicit_parts = set(gold_parts)\n",
    "x_implicit_parts = set([p.get_span().upper() for p in parts])\n",
    "print \"\\nPart Stats:\"\n",
    "print_stats(g_implicit_parts, x_implicit_parts)\n",
    "\n",
    "g_implicit_pairs = set(gold_pairs)\n",
    "x_implicit_pairs = set([(p.context.document.name, p.get_span().upper()) for p in parts])\n",
    "print \"\\nPair Stats:\"\n",
    "print_stats(g_implicit_pairs, x_implicit_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suffix Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering 1313 candidate part numbers\n",
      "\n",
      "Part Stats:\n",
      "Precision: 0.136 (179/1313)\n",
      "Recall: 1.000 (179/179)\n",
      "\n",
      "Pair Stats:\n",
      "Precision: 0.125 (541/4323)\n",
      "Recall: 0.998 (541/542)\n"
     ]
    }
   ],
   "source": [
    "g_group_parts = g_implicit_parts\n",
    "x_group_parts = set([])\n",
    "for part in x_implicit_parts:\n",
    "    for suffix in ['', 'A','B','C','-16','-25','-40']:\n",
    "        x_group_parts.update([''.join([part,suffix])])\n",
    "print \"Considering %d candidate part numbers\" % len(x_group_parts)\n",
    "        \n",
    "print \"\\nPart Stats:\"\n",
    "print_stats(g_group_parts, x_group_parts)        \n",
    "\n",
    "g_group_pairs = g_implicit_pairs\n",
    "x_group_pairs = set([])\n",
    "for (doc,part) in x_implicit_pairs:\n",
    "    for suffix in ['', 'A','B','C','-16','-25','-40']:\n",
    "        x_group_pairs.update([(doc,''.join([part.replace(' ',''),suffix]))])\n",
    "print \"\\nPair Stats:\"\n",
    "print_stats(g_group_pairs, x_group_pairs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('PJECS00521-1', 'MMBT3904')}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_group_pairs.difference(x_group_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
