{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables Total Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to get to the point where we can extract 100% of part names from transistor hardware sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 64 ms, total: 12 s\n",
      "Wall time: 12 s\n",
      "Corpus has been loaded.\n"
     ]
    }
   ],
   "source": [
    "load_pickle = True # This takes 12sec\n",
    "save_pickle = True # Saved all the documents last time. Took 30min.\n",
    "\n",
    "corpus_loaded = False\n",
    "if load_pickle:\n",
    "    try:\n",
    "        import cPickle\n",
    "        with open(\"data/hardware/hardware_corpus.pkl\",\"r\") as pkl:\n",
    "            %time corpus = cPickle.load(pkl)\n",
    "        corpus_loaded = True\n",
    "        print \"Corpus has been loaded.\"\n",
    "    except:\n",
    "        print \"Corpus could not be loaded.\"\n",
    "        print \"Corpus will be parsed instead...\"\n",
    "if not corpus_loaded:\n",
    "    from snorkel.parser import CorpusParser\n",
    "    from snorkel.parser import HTMLParser\n",
    "#     from snorkel.parser import SentenceParser\n",
    "    from snorkel.parser import TableParser\n",
    "\n",
    "    doc_parser = HTMLParser(path='data/hardware/hardware_html/')\n",
    "#     context_parser = SentenceParser()\n",
    "    context_parser = TableParser()\n",
    "\n",
    "    cp = CorpusParser(doc_parser, context_parser)\n",
    "    %time corpus = cp.parse_corpus(name='Hardware Corpus')\n",
    "    print \"Corpus has been parsed.\"\n",
    "    \n",
    "    if save_pickle:\n",
    "        with open(\"data/hardware/hardware_corpus.pkl\",\"w\") as pkl:\n",
    "            %time cPickle.dump(corpus, pkl)\n",
    "            print \"Corpus has been pickled.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 179 gold part numbers.\n"
     ]
    }
   ],
   "source": [
    "from load_dictionaries import load_hardware_dictionary\n",
    "\n",
    "gold_parts = load_hardware_dictionary()\n",
    "print \"Loaded %s gold part numbers.\" % len(gold_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 0 ns, total: 1.28 s\n",
      "Wall time: 1.25 s\n",
      "Span(\"BC546\", context=None, chars=[41,45], words=[18,18])\n",
      "Span(\"BC547\", context=None, chars=[50,54], words=[22,22])\n",
      "Span(\"BC548\", context=None, chars=[59,63], words=[26,26])\n",
      "Span(\"BC546B\", context=None, chars=[24,29], words=[11,11])\n",
      "Span(\"BC547B\", context=None, chars=[32,37], words=[13,13])\n",
      "Extracted 1111 candidate part numbers.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.candidates import Ngrams, EntityExtractor\n",
    "from snorkel.matchers import RegexMatchEach, DictionaryMatch, RangeMatcher\n",
    "\n",
    "ngrams = Ngrams(n_max=2)\n",
    "part_matcher = DictionaryMatch(d=gold_parts, longest_match_only=False)\n",
    "part_extractor = EntityExtractor(ngrams, part_matcher)\n",
    "\n",
    "%time parts = part_extractor.extract(corpus.get_sentences(), name='all')\n",
    "for p in parts[:5]: \n",
    "    print p\n",
    "print \"Extracted %s candidate part numbers.\" % len(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.799 (143/179)\n",
      "Precision: 1.000 (1/1)\n"
     ]
    }
   ],
   "source": [
    "from fractions import Fraction\n",
    "g = set(gold_parts)\n",
    "x = set([p.get_span() for p in parts])\n",
    "recall = Fraction(len(g.intersection(x)),len(g))\n",
    "precision = Fraction(len(g.intersection(x)),len(x))\n",
    "print \"Recall: %0.3f (%s/%s)\" % (float(recall), recall.numerator, recall.denominator)\n",
    "print \"Precision: %0.3f (%s/%s)\" % (float(precision), precision.numerator, precision.denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([])\n",
      "BC817-16\n",
      "2N4123\n",
      "2N4124\n",
      "BC818-16\n",
      "BC857CW\n",
      "BC817K-40W\n",
      "BC859C\n",
      "BC859B\n",
      "BC817W\n",
      "BC550C\n",
      "BC817K-25\n",
      "SMBT3904S\n",
      "DTC124EE\n",
      "BC846AW\n",
      "BC847CW\n",
      "BC846\n",
      "BC847\n",
      "BC850B\n",
      "BC850C\n",
      "BC846A\n",
      "BC846B\n",
      "BC858C-3L\n",
      "BC857A-Z3E\n",
      "BC238B\n",
      "BC858B\n",
      "BC858C\n",
      "BC846W\n",
      "BC858A\n",
      "BC327-25\n",
      "BC857B-3F\n",
      "DTC123JE\n",
      "BC846BW\n",
      "BC807-40\n",
      "BC847BW\n",
      "BC858A-3J\n",
      "2N3906\n",
      "2N3904\n",
      "2N3905\n",
      "DTC123EE\n",
      "BC182B\n",
      "BC860BW\n",
      "DTC114WKA\n",
      "BC857BL3\n",
      "BC327-16\n",
      "BC856A-3A\n",
      "DTC114EE\n",
      "BC817-40\n",
      "BC817-40W\n",
      "DTC143ZE\n",
      "BC817-25\n",
      "BC337-40\n",
      "BC859C-Z4C\n",
      "BC859A-Z4A\n",
      "BC860C-4GZ\n",
      "BC857BW\n",
      "DTC114YE\n",
      "BC860CW\n",
      "BC817K-40\n",
      "BC860B\n",
      "DTC124XE\n",
      "BC807-16W\n",
      "BC807-25\n",
      "BC818\n",
      "BC807-25W\n",
      "BC817K-16W\n",
      "BC858B-3K\n",
      "BC817\n",
      "BC859B-4B\n",
      "BC849B\n",
      "BC849C\n",
      "DTC114TE\n",
      "BC549C\n",
      "BC549B\n",
      "DTC143EE\n",
      "BC818K-16W\n",
      "MMBT3904\n",
      "MMBT3906\n",
      "BC860A-Z4E\n",
      "BC549\n",
      "BC548\n",
      "BC807-16\n",
      "DTC143TE\n",
      "BC807W\n",
      "BC547\n",
      "BC546\n",
      "BC846T\n",
      "BC807\n",
      "MMBT6427\n",
      "BC327\n",
      "BC550\n",
      "BC548A\n",
      "BC548B\n",
      "BC548C\n",
      "BC818K-40\n",
      "BC857C-3G\n",
      "BC817-16W\n",
      "BC858CW\n",
      "DTC114WUA\n",
      "BC847B\n",
      "BC847C\n",
      "DTC144EE\n",
      "BC847A\n",
      "PZT3904\n",
      "BC337-16\n",
      "PZT3906\n",
      "BC857C\n",
      "BC857B\n",
      "BC857A\n",
      "BC337-25\n",
      "BC817K-25W\n",
      "BC182A\n",
      "BC337\n",
      "BC338\n",
      "BC547A\n",
      "BC547C\n",
      "BC547B\n",
      "BC239\n",
      "BC856B-Z3B\n",
      "BC237A\n",
      "BC847AW\n",
      "BC817-25W\n",
      "BC818-40\n",
      "BC237\n",
      "MMBT4124\n",
      "BC807-40W\n",
      "BC818-25\n",
      "BC856A\n",
      "BC856B\n",
      "DTC114WE\n",
      "BC860B-4F\n",
      "BC858BW\n",
      "BC848C\n",
      "BC848B\n",
      "BC848A\n",
      "BC817K-16\n",
      "BC546B\n",
      "BC546C\n",
      "BC327-40\n",
      "BC546A\n",
      "BC183\n",
      "BC182\n",
      "BC184\n",
      "2N6426\n"
     ]
    }
   ],
   "source": [
    "print x - g\n",
    "for p in x:\n",
    "    print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fraction(1, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ranges (e.g., BC546-BC548) or lists (e.g., BC546/547/548)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] Inferred Phrases: \n",
      "  ['BC547A, ABC, D']\n",
      "3\n",
      "[debug] Final Set: \n",
      "  ['BC547A', 'BC5ABC']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from difflib import Differ, SequenceMatcher\n",
    "from pprint import pprint\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "def atoi(num_str):\n",
    "    '''\n",
    "    Converts a string to an integer, or returns None.\n",
    "    '''\n",
    "    try:\n",
    "        return int(num_str)\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# phrases = [\"BC546A/B/C...BC550A/B/C\", \"BC547A, BC5XB, C\", \"BC546A~BC546Z\", \"BC546/550/543\", \"BC547A/BC548B\", \"BC182,A,B\", \"BC546/D\"]\n",
    "phrases = [\"BC547A, ABC, D\"]\n",
    "# This range pattern will find text that \"looks like\" a range.\n",
    "range_pattern = re.compile(ur'(?P<start>[\\w/]+)(?:\\s*(\\.{3,}|\\~|\\-+|to)\\s*)(?P<end>[\\w/]+)')\n",
    "suffix_pattern = re.compile(ur'(?P<spacer>(?:,|\\/)\\s*)(?P<suffix>\\w+)')\n",
    "base_pattern = re.compile(ur'(?P<base>\\w+)(?P<spacer>(?:,|\\/)\\s*)?(?P<suffix>\\w+)?')\n",
    "for phrase in phrases:\n",
    "    inferred_phrases = set()\n",
    "    final_set = set()\n",
    "    m = re.search(range_pattern, phrase)\n",
    "    if m:\n",
    "        start = m.group(\"start\")\n",
    "        end = m.group(\"end\")\n",
    "        \n",
    "        if DEBUG:\n",
    "            print \"[debug] Start: %s \\t End: %s\" % (start, end)\n",
    "        \n",
    "        # Use difflib to find difference. We are interested in 'replace' only\n",
    "        seqm = SequenceMatcher(None, start, end).get_opcodes();\n",
    "        for opcode, a0, a1, b0, b1 in seqm:\n",
    "            if opcode == 'equal':\n",
    "                continue\n",
    "            elif opcode == 'insert':\n",
    "                break\n",
    "            elif opcode == 'delete':\n",
    "                break\n",
    "            elif opcode == 'replace':\n",
    "                # NOTE: Potential bug if there is more than 1 replace\n",
    "                start_diff = start[a0:a1]\n",
    "                end_diff = end[b0:b1]\n",
    "            else:\n",
    "                raise RuntimeError, \"[ERROR] unexpected opcode\"\n",
    "\n",
    "\n",
    "        if DEBUG: print \"[debug] start_diff: %s \\t end_diff: %s\" % (start_diff, end_diff)\n",
    "\n",
    "        # Check Numbers\n",
    "        if atoi(start_diff) and atoi(end_diff):\n",
    "            if DEBUG: print \"[debug] Enumerate %d to %d\" % (start_num, end_num)\n",
    "            # generate a list of the numbers plugged in\n",
    "            number_range = range(atoi(start_diff), atoi(end_diff) + 1)\n",
    "            for number in number_range:\n",
    "                new_phrase = start.replace(start_diff,str(number))\n",
    "                # Produce the strings with the enumerated ranges\n",
    "                inferred_phrases.add(new_phrase)\n",
    "\n",
    "        # Second, check for single-letter enumeration\n",
    "        if len(start_diff) == 1 and len(end_diff) == 1:\n",
    "            if start_diff.isalpha() and end_diff.isalpha():\n",
    "                def char_range(a, b):\n",
    "                    '''\n",
    "                    Generates the characters from a to b inclusive.\n",
    "                    '''\n",
    "                    for c in xrange(ord(a), ord(b)+1):\n",
    "                        yield chr(c)\n",
    "                \n",
    "                if DEBUG: print \"[debug] Enumerate %s to %s\" % (start_diff, end_diff)\n",
    "                letter_range = char_range(start_diff, end_diff)\n",
    "                for letter in letter_range:\n",
    "                    new_phrase = start.replace(start_diff,letter)\n",
    "                    # Produce the strings with the enumerated ranges\n",
    "                    inferred_phrases.add(new_phrase)\n",
    "    else: inferred_phrases.add(phrase)\n",
    "    if DEBUG: print \"[debug] Inferred Phrases: \\n  \" + str(sorted(inferred_phrases))\n",
    "    \n",
    "    # Handle lists for each of the inferred phrases\n",
    "    # NOTE: this only does the simple case of replacing same-length suffixes.\n",
    "    # we do not handle cases like \"BC546A/B/XYZ/QR\"\n",
    "    for phrase in inferred_phrases:\n",
    "        first_match = re.search(base_pattern,phrase)\n",
    "        if first_match: \n",
    "            base = re.search(base_pattern,phrase).group(\"base\");\n",
    "            final_set.add(base) # add the base (multiple times, but set handles that)\n",
    "            if (first_match.group(\"suffix\")):\n",
    "                all_suffix_lengths = set()\n",
    "                for m in re.finditer(suffix_pattern, phrase):\n",
    "                    suffix = m.group(\"suffix\")\n",
    "                    suffix_len = len(suffix)\n",
    "                if all_suffix_lengths\n",
    "                for m in re.finditer(suffix_pattern, phrase):\n",
    "                    spacer = m.group(\"spacer\")\n",
    "                    suffix = m.group(\"suffix\")\n",
    "                    suffix_len = len(suffix)\n",
    "                    if prev_suffix_len != suffix_len:\n",
    "                        break # suffixes aren't same length\n",
    "\n",
    "                    trimmed = base[:-suffix_len]\n",
    "                    final_set.add(trimmed+suffix)\n",
    "    if DEBUG: print \"[debug] Final Set: \\n  \" + str(sorted(final_set))\n",
    "\n",
    "    # NOTE: We make a few assumptions (e.g. suffixes must be same length), but\n",
    "    # one important unstated assumption is that if there is a single suffix,\n",
    "    # (e.g. BC546A/B), the single suffix will be swapped in no matter what.\n",
    "    # In this example, it works. But if we had \"ABCD/EFG\" we would get \"ABCD,AEFG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
